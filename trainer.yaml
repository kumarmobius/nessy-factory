name: LM Trainer
description: Tokenizes corpus to memmaps and trains a Gemma3-compatible model (Gemma or GPT-NeoX alias) with warmupâ†’cosine LR, grad accumulation, and best/final checkpoints.

inputs:
  - name: tokenizer_json
    type: Model
  - name: train_corpus
    type: Data
  - name: model_config
    type: Data
  - name: model_weights
    type: Model
  - name: model_py_in
    type: Data

  # Hyperparameters
  - name: learning_rate
    type: String
    default: "1e-4"
  - name: min_lr
    type: String
    default: "1e-5"
  - name: warmup_steps
    type: Integer
    default: "1000"
  - name: max_iters
    type: Integer
    default: "150000"
  - name: batch_size
    type: Integer
    default: "32"
  - name: block_size
    type: Integer
    default: "128"
  - name: grad_accum
    type: Integer
    default: "32"
  - name: eval_interval
    type: Integer
    default: "1000"
  - name: eval_iters
    type: Integer
    default: "500"
  - name: weight_decay
    type: String
    default: "0.1"
  - name: beta2
    type: String
    default: "0.95"
  - name: clip_grad_norm
    type: String
    default: "0.5"
  - name: val_fraction
    type: String
    default: "0.1"
  - name: num_proc
    type: Integer
    default: "8"

outputs:
  - name: best_weights
    type: Model
  - name: final_weights
    type: Model
  - name: training_report
    type: Data
  - name: loss_curve_csv
    type: Data
  - name: model_py
    type: Data
  - name: schema_json
    type: String

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v21
    command:
      - bash
      - -eu
      - -c
      - |-
        set -o pipefail

        echo "[DEBUG] shell received args: $*"

        cat >/tmp/train_with_gemmatrainer.py <<'PY'
        import argparse, json, os, shutil, sys, csv, math, glob, logging
        from pathlib import Path

        DEFAULTS = {
            "tokenizer_json": "/tmp/inputs/tokenizer_json/data",
            "train_corpus":   "/tmp/inputs/train_corpus/data",
            "model_config":   "/tmp/inputs/model_config/data",
            "model_weights":  "/tmp/inputs/model_weights/data",
            "model_py_in":    "/tmp/inputs/model_py_in/data",
            "best_weights":   "/tmp/outputs/best_weights/data",
            "final_weights":  "/tmp/outputs/final_weights/data",
            "training_report":"/tmp/outputs/training_report/data",
            "loss_curve_csv": "/tmp/outputs/loss_curve_csv/data",
            "model_py_out":   "/tmp/outputs/model_py/data",
            "schema_output":  "/tmp/outputs/schema_json/data",
        }

        def _pick_model_py(path_str: str) -> str:
            p = Path(path_str)
            if p.is_file():
                return str(p)
            if not p.exists():
                raise FileNotFoundError(f"model_py_in not found: {p}")
            for cand in p.iterdir():
                if cand.suffix == ".py":
                    return str(cand)
            raise FileNotFoundError(f"No .py file under directory: {p}")

        def _exist(p): return Path(p).exists()

        def _head(path, nlines=20):
            try:
                with open(path, "r", encoding="utf-8") as f:
                    lines = []
                    for i, ln in enumerate(f):
                        lines.append(ln.rstrip("\n"))
                        if i+1 >= nlines:
                            break
                    return "\n".join(lines)
            except Exception as e:
                logging.error('could not read file: %s %s', path, e)
                return None

        def _is_number(x):
            try:
                float(x)
                return True
            except Exception:
                return False

        # robust extract from JSON structures
        def _extract_curve_from_obj(obj):
            # list of numbers
            if isinstance(obj, list) and all(isinstance(x, (int, float)) for x in obj):
                return [float(x) for x in obj]
            # list of dicts -> common keys
            if isinstance(obj, list) and obj and isinstance(obj[0], dict):
                for k in ("loss","value","y","train_loss","val_loss","validation_loss"):
                    if all((k in it) for it in obj):
                        try:
                            return [float(it.get(k, math.nan)) for it in obj]
                        except Exception:
                            pass
                first = obj[0]
                for k, v in first.items():
                    if isinstance(v, (int, float)):
                        try:
                            return [float(it.get(k, math.nan)) for it in obj]
                        except Exception:
                            pass
            # list of pairs
            if isinstance(obj, list) and obj and isinstance(obj[0], (list, tuple)) and len(obj[0]) >= 2:
                try:
                    return [float(x[-1]) for x in obj]
                except Exception:
                    pass
            return []

        def _extract_from_csv(path):
            out_loss = []
            out_val = []
            try:
                with open(path, "r", encoding="utf-8") as f:
                    reader = csv.DictReader(f)
                    # if headers present
                    if reader.fieldnames:
                        # normalize header names
                        h = [c.strip().lower() for c in reader.fieldnames]
                        # decide column names
                        loss_col = None
                        val_col = None
                        for c in reader.fieldnames:
                            cl = c.strip().lower()
                            if cl in ("loss","train_loss","trainloss","train"):
                                loss_col = c
                            if cl in ("val_loss","validation_loss","valloss","val"):
                                val_col = c
                        # fallback heuristics
                        rows = list(reader)
                        if loss_col:
                            out_loss = [float(r.get(loss_col)) for r in rows if _is_number(r.get(loss_col))]
                        if val_col:
                            out_val = [float(r.get(val_col)) for r in rows if _is_number(r.get(val_col))]
                        # if no headers mapped, try first two numeric columns
                        if not loss_col and rows:
                            # try to pick first numeric column
                            for col in reader.fieldnames:
                                col_vals = [r.get(col) for r in rows]
                                if all(_is_number(v) for v in col_vals if v is not None):
                                    out_loss = [float(v) for v in col_vals if v is not None]
                                    break
            except Exception as e:
                print(f"[DEBUG] CSV parse failed for {path}: {e}")
            return out_loss, out_val

        def main():
            ap = argparse.ArgumentParser()
            ap.add_argument("--tokenizer-json", default=None)
            ap.add_argument("--train-corpus",   default=None)
            ap.add_argument("--model-config",   default=None)
            ap.add_argument("--model-weights",  default=None)
            ap.add_argument("--model-py-in",    default=None)
            ap.add_argument("--model-py-out",   default=None)

            ap.add_argument("--learning-rate", type=float, required=True)
            ap.add_argument("--min-lr", type=float, required=True)
            ap.add_argument("--warmup-steps", type=int, required=True)
            ap.add_argument("--max-iters", type=int, required=True)
            ap.add_argument("--batch-size", type=int, required=True)
            ap.add_argument("--block-size", type=int, required=True)
            ap.add_argument("--grad-accum", type=int, required=True)
            ap.add_argument("--eval-interval", type=int, required=True)
            ap.add_argument("--eval-iters", type=int, required=True)
            ap.add_argument("--weight-decay", type=float, required=True)
            ap.add_argument("--beta2", type=float, required=True)
            ap.add_argument("--clip-grad-norm", type=float, required=True)
            ap.add_argument("--val-fraction", type=float, required=True)
            ap.add_argument("--num-proc", type=int, required=True)

            ap.add_argument("--best-weights",   default=None)
            ap.add_argument("--final-weights",  default=None)
            ap.add_argument("--training-report",default=None)
            ap.add_argument("--loss-curve-csv", default=None)
            ap.add_argument("--schema-output",  default=None)
            args = ap.parse_args()

            # fallbacks
            for k in ("tokenizer_json","train_corpus","model_config","model_weights","model_py_in",
                      "best_weights","final_weights","training_report","loss_curve_csv","model_py_out","schema_output"):
                if getattr(args, k.replace("-", "_")) in (None, ""):
                    setattr(args, k.replace("-", "_"), DEFAULTS[k])

            print("[DEBUG] resolved paths:")
            for k in ("tokenizer_json","train_corpus","model_config","model_weights","model_py_in",
                      "best_weights","final_weights","training_report","loss_curve_csv","model_py_out","schema_output"):
                v = getattr(args, k.replace("-", "_"))
                print(f"  {k:16s} -> {v} (exists={_exist(v)})")

            # ensure output dirs
            for p in (args.best_weights, args.final_weights, args.training_report,
                      args.loss_curve_csv, args.model_py_out, args.schema_output):
                d = os.path.dirname(p)
                if d:
                    os.makedirs(d, exist_ok=True)

            src_py = _pick_model_py(args.model_py_in)
            dst = args.model_py_out
            if os.path.isdir(dst) or dst.endswith("/data"):
                os.makedirs(dst, exist_ok=True)
                dst_file = os.path.join(dst, "model.py")
            else:
                os.makedirs(os.path.dirname(dst) or ".", exist_ok=True)
                dst_file = dst
            try:
                same = os.path.exists(src_py) and os.path.exists(dst_file) and os.path.samefile(src_py, dst_file)
            except Exception:
                same = os.path.abspath(src_py) == os.path.abspath(dst_file)
            if same:
                dst_file = os.path.join(os.path.dirname(dst_file), "model_copy.py")
            shutil.copyfile(src_py, dst_file)


            try:
                from nesy_factory.language_model.train import GemmaTrainer
            except Exception as e:
                print(f"[ERROR] Could not import GemmaTrainer: {e}")
                raise

            trainer = GemmaTrainer()
            trainer.run(
                tokenizer_json=args.tokenizer_json,
                train_corpus=args.train_corpus,
                model_config=args.model_config,
                model_weights=args.model_weights,
                model_py_in=src_py,
                model_py_out=dst_file,
                learning_rate=args.learning_rate,
                min_lr=args.min_lr,
                warmup_steps=args.warmup_steps,
                max_iters=args.max_iters,
                batch_size=args.batch_size,
                block_size=args.block_size,
                grad_accum=args.grad_accum,
                eval_interval=args.eval_interval,
                eval_iters=args.eval_iters,
                weight_decay=args.weight_decay,
                beta2=args.beta2,
                clip_grad_norm=args.clip_grad_norm,
                val_fraction=args.val_fraction,
                num_proc=args.num_proc,
                best_weights=args.best_weights,
                final_weights=args.final_weights,
                training_report=args.training_report,
                loss_curve_csv=args.loss_curve_csv,
            )

            # ------------- build schema -------------
            report = {}
            if os.path.exists(args.training_report):
                try:
                    with open(args.training_report, "r", encoding="utf-8") as f:
                        report = json.load(f)
                    print("[DEBUG] training_report preview:")
                    print(json.dumps(report if isinstance(report, dict) else {"_report_type": type(report).__name__}, indent=2)[:2000])
                except Exception as e:
                    print(f"[WARN] could not load training_report JSON: {e}")

            train_curve = []
            val_curve = []

            # Common keys to try
            def try_extract_from_report(rep):
                out_train = []
                out_val = []
                # try direct keys
                for k in ("train_curve","train_losses","train_loss","train","losses"):
                    if k in rep:
                        out_train = _extract_curve_from_obj(rep[k])
                        if out_train:
                            break
                for k in ("val_curve","val_losses","val_loss","validation_curve","validation_loss","val_history"):
                    if k in rep:
                        out_val = _extract_curve_from_obj(rep[k])
                        if out_val:
                            break
                # nested
                if not out_train and isinstance(rep.get("train"), dict):
                    out_train = _extract_curve_from_obj(rep["train"].get("losses") or rep["train"].get("loss") or rep["train"])
                if not out_val and isinstance(rep.get("val"), dict):
                    out_val = _extract_curve_from_obj(rep["val"].get("losses") or rep["val"].get("loss") or rep["val"])
                return out_train, out_val

            if isinstance(report, dict) and report:
                t, v = try_extract_from_report(report)
                train_curve.extend(t or [])
                val_curve.extend(v or [])

            # if still empty, try loss_curve_csv
            if (not train_curve) and os.path.exists(args.loss_curve_csv):
                print(f"[DEBUG] attempting to parse CSV: {args.loss_curve_csv}")
                l, v = _extract_from_csv(args.loss_curve_csv)
                train_curve = train_curve or l
                val_curve = val_curve or v

            # if still empty, scan output dir for likely JSON/CSV files
            if (not train_curve) and (not val_curve):
                outdir = os.path.dirname(args.training_report) or "."
                candidates = []
                for ext in ("*.json","*.ndjson","*.csv"):
                    candidates.extend(glob.glob(os.path.join(outdir, ext)))
                candidates = [c for c in candidates if not c.endswith(os.path.basename(args.training_report))]
                for c in candidates:
                    try:
                        if c.endswith(".csv"):
                            l, v = _extract_from_csv(c)
                            if l and not train_curve:
                                train_curve = l
                            if v and not val_curve:
                                val_curve = v
                        else:
                            with open(c, "r", encoding="utf-8") as f:
                                data = json.load(f)
                            # try various shapes
                            if isinstance(data, dict):
                                t, v = try_extract_from_report(data)
                                if t and not train_curve:
                                    train_curve = t
                                if v and not val_curve:
                                    val_curve = v
                            elif isinstance(data, list):
                                # list of numbers?
                                nums = _extract_curve_from_obj(data)
                                if nums and not train_curve:
                                    train_curve = nums
                    except Exception as e:
                        print(f"[DEBUG] scanning {c} failed: {e}")

            # finalize to floats
            train_curve = [float(x) for x in train_curve] if train_curve else []
            val_curve = [float(x) for x in val_curve] if val_curve else []

            max_len = max(len(train_curve), len(val_curve))
            schema_list = []
            for i in range(max_len):
                tr = train_curve[i] if i < len(train_curve) else None
                va = val_curve[i] if i < len(val_curve) else None
                if tr is None and va is None:
                    continue
                schema_list.append({
                    "epoch": i + 1,
                    "loss": float(tr) if tr is not None else None,
                    "validation_loss": float(va) if va is not None else None
                })

            # --- Save schema_json the same way as your inference job ---
            schema_out = args.schema_output
            # if user passed a directory, append filename
            if os.path.isdir(schema_out):
                schema_out = os.path.join(schema_out, "schema.json")
            os.makedirs(os.path.dirname(schema_out) or ".", exist_ok=True)

            # Print schema to logs for debugging
            print("[DEBUG] schema_list to be written (preview):")
            print(json.dumps(schema_list, indent=2)[:4000])

            with open(schema_out, "w", encoding="utf-8") as f:
                json.dump(schema_list, f, indent=2, ensure_ascii=False)
            print(f"[INFO] Wrote schema_json to: {schema_out} (entries={len(schema_list)})")

        if __name__ == "__main__":
            main()
        PY

        python3 -u /tmp/train_with_gemmatrainer.py "$@"
      - _
    args:
      - --tokenizer-json
      - {inputPath: tokenizer_json}
      - --train-corpus
      - {inputPath: train_corpus}
      - --model-config
      - {inputPath: model_config}
      - --model-weights
      - {inputPath: model_weights}
      - --model-py-in
      - {inputPath: model_py_in}

      - --learning-rate
      - {inputValue: learning_rate}
      - --min-lr
      - {inputValue: min_lr}
      - --warmup-steps
      - {inputValue: warmup_steps}
      - --max-iters
      - {inputValue: max_iters}
      - --batch-size
      - {inputValue: batch_size}
      - --block-size
      - {inputValue: block_size}
      - --grad-accum
      - {inputValue: grad_accum}
      - --eval-interval
      - {inputValue: eval_interval}
      - --eval-iters
      - {inputValue: eval_iters}
      - --weight-decay
      - {inputValue: weight_decay}
      - --beta2
      - {inputValue: beta2}
      - --clip-grad-norm
      - {inputValue: clip_grad_norm}
      - --val-fraction
      - {inputValue: val_fraction}
      - --num-proc
      - {inputValue: num_proc}

      - --best-weights
      - {outputPath: best_weights}
      - --final-weights
      - {outputPath: final_weights}
      - --training-report
      - {outputPath: training_report}
      - --loss-curve-csv
      - {outputPath: loss_curve_csv}
      - --model-py-out
      - {outputPath: model_py}
      - --schema-output
      - {outputPath: schema_json}
