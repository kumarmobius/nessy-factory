name: Benchmarks
description: |
  Run multiple lm-eval benchmarks (gpqa, mmlu_pro, bbh, math, ifeval) on a converted
  Hugging Face model folder. Accepts converted_model (HF folder) and hf_token (string).
  Returns compact + full JSON results and an emissions CSV.

inputs:
  - name: converted_model
    type: Model
    description: Path to converted HF model folder (artifact directory)
  - name: hf_token
    type: String
    default: ""
    description: Hugging Face token (optional, if dataset is gated)
  - name: cache_dir
    type: String
    default: /tmp/hf_cache
    description: HF cache dir for datasets / hub
  - name: tasks
    type: String
    default: "gpqa,mmlu_pro,bbh,math,ifeval"
    description: Comma-separated lm-eval task names
  - name: max_new_tokens
    type: Integer
    default: "64"
    description: Max new tokens for generation (passed as context to LM when needed)
  - name: out_compact
    type: Data
    default: /tmp/outputs/benchmark_compact.json
  - name: out_full
    type: Data
    default: /tmp/outputs/benchmark_full.json
  - name: out_emissions
    type: Data
    default: /tmp/outputs/emissions.csv

outputs:
  - name: compact_metrics
    type: Data
  - name: full_results
    type: Data
  - name: emissions_csv
    type: Data
  - name: logs
    type: Data

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v23gpu
    command:
      - bash
      - -lc
      - |
        set -euo pipefail
        echo "[BOOT] Installing required Python packages (lm_eval, codecarbon)..."
        PIP="python3 -m pip"
        # Try system-wide quiet install first; if it fails (permissions), retry with --user
        PIP_DISABLE_PIP_VERSION_CHECK=1 $PIP install --quiet --no-warn-script-location lm_eval codecarbon \
          || PIP_DISABLE_PIP_VERSION_CHECK=1 $PIP install --user --quiet --no-warn-script-location lm_eval codecarbon
        export PATH="$HOME/.local/bin:$PATH"
        echo "[BOOT] Writing eval script to /tmp/eval_benchmarks.py"

        cat > /tmp/eval_benchmarks.py <<'PY'
        #!/usr/bin/env python3
        import argparse
        import os
        import json
        import time
        import traceback
        import numpy as np
        import torch
        from huggingface_hub import HfFolder
        from datasets import load_dataset
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from lm_eval import evaluator
        from codecarbon import EmissionsTracker
        
        parser = argparse.ArgumentParser(description="Evaluate HF model on multiple lm_eval tasks")
        parser.add_argument("--converted_model", required=True)
        parser.add_argument("--hf_token", default="")
        parser.add_argument("--cache_dir", default="/tmp/hf_cache")
        parser.add_argument("--tasks", default="gpqa,mmlu_pro,bbh,math,ifeval")
        parser.add_argument("--max_new_tokens", type=int, default=64)
        parser.add_argument("--out_compact", default="/tmp/outputs/benchmark_compact.json")
        parser.add_argument("--out_full", default="/tmp/outputs/benchmark_full.json")
        parser.add_argument("--out_emissions", default="/tmp/outputs/emissions.csv")
        args = parser.parse_args()
        
        CONVERTED = os.path.abspath(args.converted_model)
        CACHE = os.path.abspath(args.cache_dir)
        OUT_COMPACT = os.path.abspath(args.out_compact)
        OUT_FULL = os.path.abspath(args.out_full)
        OUT_EMISSIONS = os.path.abspath(args.out_emissions)
        
        # Make sure output dirs exist
        for p in [os.path.dirname(OUT_COMPACT), os.path.dirname(OUT_FULL), os.path.dirname(OUT_EMISSIONS), CACHE]:
            if p:
                os.makedirs(p, exist_ok=True)
        
        TOKEN = args.hf_token or ""
        if TOKEN:
            os.environ["HUGGINGFACE_HUB_TOKEN"] = TOKEN
            os.environ["HF_TOKEN"] = TOKEN
            HfFolder.save_token(TOKEN)
            print("[EVAL] Saved HF token for gated dataset access")
        
        os.environ.update({
            "HF_HUB_DISABLE_TELEMETRY": "1",
            "HF_HOME": CACHE,
            "HF_DATASETS_CACHE": CACHE,
            "HUGGINGFACE_HUB_CACHE": CACHE
        })
        
        TASKS = [t.strip() for t in args.tasks.split(",") if t.strip()]
        COMPOSITE_GROUP = ["mmlu_pro", "bbh", "math", "ifeval"]
        
        PRELOAD_MAP = {
            "gpqa": [("Idavidrein/gpqa", "gpqa_main")],
        }
        
        def _preload_for_task(task):
            task_key = task.lower()
            entries = PRELOAD_MAP.get(task_key)
            if not entries:
                return False
            ok_any = False
            for repo, split in entries:
                try:
                    print("[EVAL] Preloading {} split={} ...".format(repo, split))
                    try:
                        load_dataset(repo, split, cache_dir=CACHE, token=TOKEN)
                    except TypeError:
                        load_dataset(repo, split, cache_dir=CACHE, use_auth_token=TOKEN)
                    print("[EVAL] Preloaded {}::{}".format(repo, split))
                    ok_any = True
                except Exception as e:
                    print("[EVAL] Preload failed for {}::{}: {}".format(repo, split, e))
            return ok_any
        
        # Start CO2 tracker; file will be created in OUT_EMISSIONS dir
        emissions_out_dir = os.path.dirname(OUT_EMISSIONS) or "."
        tracker = EmissionsTracker(output_dir=emissions_out_dir, output_file=os.path.basename(OUT_EMISSIONS))
        tracker.start()
        
        print("[EVAL] Loading tokenizer + model from:", CONVERTED)
        tokenizer = AutoTokenizer.from_pretrained(CONVERTED, trust_remote_code=False)
        _ = AutoModelForCausalLM.from_pretrained(CONVERTED, trust_remote_code=False)
        
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            print("[EVAL] CUDA visible devices: {}".format(os.environ.get("CUDA_VISIBLE_DEVICES", "all")))
            print("[EVAL] torch reports {} CUDA device(s).".format(gpu_count))
            for i in range(gpu_count):
                try:
                    name = torch.cuda.get_device_name(i)
                except Exception:
                    name = "<unknown>"
                try:
                    cap = torch.cuda.get_device_capability(i)
                except Exception:
                    cap = ("?", "?")
                print("  GPU {}: {}  |  capability: {}".format(i, name, cap))
            eval_device = "cuda:0"
        else:
            print("[EVAL] CUDA not available â€” using CPU")
            eval_device = "cpu"
        print("[EVAL] Using device string for lm_eval: {}".format(eval_device))
        
        def safe_json(obj):
            if isinstance(obj, dict):
                return {k: safe_json(v) for k, v in obj.items()}
            if isinstance(obj, (list, tuple)):
                return [safe_json(v) for v in obj]
            try:
                if hasattr(obj, "item"):
                    return obj.item()
            except Exception:
                pass
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            if isinstance(obj, (np.dtype, torch.dtype, torch.device, type)):
                return str(obj)
            try:
                json.dumps(obj)
                return obj
            except Exception:
                return str(obj)
        
        def pick_metric(d):
            for k in ("acc_norm", "acc", "accuracy"):
                if k in d and isinstance(d[k], (int, float)):
                    return k, float(d[k])
            for k, v in d.items():
                if isinstance(v, (int, float)):
                    return k, float(v)
            return None, None
        
        compact_results = {}
        full_results = {}
        
        # for task in TASKS:
        #     print("\n[EVAL] moddak: {}".format(task))
        #     _preload_for_task(task)
        
        #     start_t = time.time()
        #     try:
        #         model_args_str = "pretrained={},tokenizer={},trust_remote_code=False".format(CONVERTED, CONVERTED)
        #         res = evaluator.simple_evaluate(
        #             model="hf",
        #             model_args=model_args_str,
        #             tasks=[task],
        #             batch_size="auto:1",
        #             device=eval_device,
        #             num_fewshot=0,
        #             limit=None,
        #         )
        #         duration = time.time() - start_t
        #         full_results[task] = safe_json(res)
        
        #         results_dict = res.get("results", {})
        #         task_result = results_dict.get(task) or (next(iter(results_dict.values())) if results_dict else None)
        
        #         if isinstance(task_result, dict):
        #             metric_name, metric_value = pick_metric(task_result)
        #             if metric_name is not None:
        #                 compact_results[task] = {metric_name: metric_value, "time_s": round(duration, 3)}
        #                 print("[EVAL] {}: {} = {}  (time {:.1f}s)".format(task, metric_name, metric_value, duration))
        #             else:
        #                 compact_results[task] = {"note": "no numeric metric found", "time_s": round(duration, 3)}
        #                 print("[EVAL] {}: no numeric metric found  (time {:.1f}s)".format(task, duration))
        #         else:
        #             compact_results[task] = {"note": "unexpected result structure", "raw": safe_json(task_result), "time_s": round(duration,3)}
        #             print("[EVAL] {}: unexpected result structure  (time {:.1f}s)".format(task, duration))
        
        #     except Exception as e:
        #         duration = time.time() - start_t
        #         tb = traceback.format_exc()
        #         print("[EVAL] Task {} failed with exception:\n{}".format(task, tb))
        #         compact_results[task] = {"error": str(e), "time_s": round(duration, 3)}
        #         full_results[task] = {"error": str(e), "traceback": tb, "time_s": round(duration, 3)}
        
        # Stop emissions tracker
        emissions = tracker.stop()
        compact_results["co2_emissions_kg"] = float(emissions)
        full_results["co2_emissions_kg"] = float(emissions)
        
        # Compute composite mean
        found_vals = []
        for t in COMPOSITE_GROUP:
            entry = compact_results.get(t)
            if isinstance(entry, dict):
                for k, v in entry.items():
                    if k == "time_s" or k.startswith("note") or k == "error":
                        continue
                    if isinstance(v, (int, float)):
                        found_vals.append(float(v))
                        break
        if found_vals:
            composite_score = float(np.mean(found_vals))
            compact_results["composite_mean"] = round(composite_score, 6)
            print("\n[EVAL] Composite mean over {}: {:.6f}".format(COMPOSITE_GROUP, composite_score))
        else:
            print("\n[EVAL] No numeric values found for composite computation.")
        
        # Write outputs
        with open(OUT_COMPACT, "w") as f:
            json.dump(safe_json(compact_results), f, indent=2)
        with open(OUT_FULL, "w") as f:
            json.dump(safe_json(full_results), f, indent=2)
        
        print("[EVAL] Saved compact summary to:", OUT_COMPACT)
        print("[EVAL] Saved full results to:", OUT_FULL)
        print("[EVAL] Emissions CSV path (written by codecarbon):", OUT_EMISSIONS)
        PY

        echo "[BOOT] Running evaluation script..."
        python3 -u /tmp/eval_benchmarks.py "$@"
    args:
      - --converted_model
      - {inputPath: converted_model}
      - --hf_token
      - {inputValue: hf_token}
      - --cache_dir
      - {inputValue: cache_dir}
      - --tasks
      - {inputValue: tasks}
      - --max_new_tokens
      - {inputValue: max_new_tokens}
      - --out_compact
      - {outputPath: compact_metrics}
      - --out_full
      - {outputPath: full_results}
      - --out_emissions
      - {outputPath: emissions_csv}
      - {outputPath: logs}
