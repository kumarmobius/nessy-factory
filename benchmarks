name: benchmark
description: |
  Convert a Gemma3 / GPT-NeoX checkpoint + tokenizer/config into Hugging Face model folder,
  then run per-task benchmarks (mmlu_pro, bbh, math, ifeval, gpqa) and return compact metrics + CO2.

inputs:
  - name: tokenizer_json
    type: Model
  - name: model_pt
    type: Model
  - name: model_config_json
    type: Data
  - name: model_py_in
    type: Data
  - name: hf_token
    type: String
    default: ""
  - name: tasks
    type: String
    default: "mmlu_pro,bbh,math,ifeval,gpqa"
  - name: cache_dir
    type: String
    default: "/tmp/hf_cache"
  - name: output_model_dir
    type: String
    default: "/tmp/hf_model"
  - name: max_new_tokens
    type: Integer
    default: "64"

outputs:
  - name: converted_model
    type: Model
  - name: compact_metrics_json
    type: Data
  - name: full_results_json
    type: Data
  - name: emissions_csv
    type: Data
  - name: logs
    type: Data

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v23gpu
    command:
      - bash
      - -eu
      - -c
      - |-
        set -o pipefail
        echo "[PIPELINE] Starting convert + benchmark pipeline"
        echo "[PIPELINE] args: $*"

        # Expected call order when pipeline executes this container:
        # 1: hf_token (can be empty)
        # 2: tokenizer_json inputPath
        # 3: model_pt inputPath
        # 4: model_config_json inputPath
        # 5: model_py_in inputPath
        # 6: tasks (string)
        # 7: cache_dir
        # 8: output_model_dir
        # 9: max_new_tokens
        # 10: out_compact path
        # 11: out_full path
        # 12: out_emissions path
        # 13: log path

        HF_TOKEN_ARG="${1:-}"
        TOKENIZER_PATH="${2:-}"
        PT_PATH="${3:-}"
        CFG_JSON_PATH="${4:-}"
        MODEL_PY_IN="${5:-}"
        TASKS_ARG="${6:-mmlu_pro,bbh,math,ifeval,gpqa}"
        CACHE_DIR="${7:-/tmp/hf_cache}"
        OUT_MODEL_DIR="${8:-/tmp/hf_model}"
        MAX_NEW_TOKENS="${9:-64}"
        OUT_COMPACT="${10:-/tmp/metrics_compact.json}"
        OUT_FULL="${11:-/tmp/metrics_full.json}"
        OUT_EMISSIONS="${12:-/tmp/emissions.csv}"
        LOGFILE="${13:-/tmp/convert_and_eval.log}"

        mkdir -p "$CACHE_DIR" "$OUT_MODEL_DIR" "$(dirname "$OUT_COMPACT")" "$(dirname "$OUT_FULL")" "$(dirname "$OUT_EMISSIONS")"
        echo "[PIPELINE] CACHE_DIR=$CACHE_DIR OUT_MODEL_DIR=$OUT_MODEL_DIR" | tee -a "$LOGFILE"

        # Save token to env if provided (used by datasets/hf hub)
        if [ -n "$HF_TOKEN_ARG" ]; then
          export HUGGINGFACE_HUB_TOKEN="$HF_TOKEN_ARG"
          export HF_TOKEN="$HF_TOKEN_ARG"
          echo "[PIPELINE] Saved HF token" | tee -a "$LOGFILE"
        fi

        # Copy inputs to known locations (pipeline may already mount them)
        if [ -n "$TOKENIZER_PATH" ] && [ -f "$TOKENIZER_PATH" ]; then
          cp "$TOKENIZER_PATH" "/tmp/tokenizer.json"
          echo "[PIPELINE] Copied tokenizer.json -> /tmp/tokenizer.json" | tee -a "$LOGFILE"
        fi

        if [ -n "$PT_PATH" ] && [ -f "$PT_PATH" ]; then
          cp "$PT_PATH" /tmp/final.pt
          echo "[PIPELINE] Copied model_pt -> /tmp/final.pt" | tee -a "$LOGFILE"
        else
          echo "[PIPELINE][ERROR] model_pt not found at $PT_PATH" | tee -a "$LOGFILE"
          exit 2
        fi

        if [ -n "$CFG_JSON_PATH" ] && [ -f "$CFG_JSON_PATH" ]; then
          cp "$CFG_JSON_PATH" /tmp/gptneox_config.json
          echo "[PIPELINE] Copied model_config_json -> /tmp/gptneox_config.json" | tee -a "$LOGFILE"
        fi

        if [ -n "$MODEL_PY_IN" ]; then
          if [ -f "$MODEL_PY_IN" ]; then
            cp "$MODEL_PY_IN" /tmp/model_py_in.py
            echo "[PIPELINE] Copied model_py_in -> /tmp/model_py_in.py" | tee -a "$LOGFILE"
          elif [ -d "$MODEL_PY_IN" ]; then
            cp -r "$MODEL_PY_IN" /tmp/model_py_in
            echo "[PIPELINE] Copied model_py_in dir -> /tmp/model_py_in" | tee -a "$LOGFILE"
          fi
        fi

        ###############################################################################
        # 1) Write converter script (uses argparse)
        ###############################################################################
        cat >/tmp/build_hf_neox_folder_dynamic.py <<'PY'
        #!/usr/bin/env python3
        import argparse, os, re, json, shutil, torch
        from collections import OrderedDict

        def main():
            ap = argparse.ArgumentParser()
            ap.add_argument("--src-pt", required=True, help="Path to final .pt checkpoint")
            ap.add_argument("--tokenizer-json", default=None, help="Optional tokenizer.json")
            ap.add_argument("--base-cfg-json", default=None, help="Optional gptneox config JSON")
            ap.add_argument("--out-dir", required=True, help="Output HF model dir")
            args = ap.parse_args()

            SRC_PT = args.src_pt
            TOKENIZER_JSON = args.tokenizer_json
            BASE_CFG_JSON = args.base_cfg_json
            MODEL_DIR = args.out_dir

            os.makedirs(MODEL_DIR, exist_ok=True)
            print("[CONVERT] Loading checkpoint:", SRC_PT)
            sd = torch.load(SRC_PT, map_location="cpu")
            if isinstance(sd, dict) and "state_dict" in sd:
                sd = sd["state_dict"]
            # best-effort: accept different key namespaces
            out = OrderedDict(sd)

            # rename final layernorm keys if necessary
            if "gpt_neox.final_layernorm.weight" in out and "gpt_neox.final_layer_norm.weight" not in out:
                out["gpt_neox.final_layer_norm.weight"] = out.pop("gpt_neox.final_layernorm.weight")
                if "gpt_neox.final_layernorm.bias" in out:
                    out["gpt_neox.final_layer_norm.bias"]   = out.pop("gpt_neox.final_layernorm.bias")

            # ensure embed_out exists or mirror from embed_in
            if "gpt_neox.embed_out.weight" not in out and "gpt_neox.embed_in.weight" in out:
                out["gpt_neox.embed_out.weight"] = out["gpt_neox.embed_in.weight"]

            # add missing biases for attention and dense layers
            layer_ids = sorted({int(m.group(1)) for k in out.keys()
                                for m in [re.match(r"gpt_neox\.layers\.(\d+)\.attention\.query_key_value\.weight$", k)] if m})
            for i in layer_ids:
                wq = out.get(f"gpt_neox.layers.{i}.attention.query_key_value.weight")
                wo = out.get(f"gpt_neox.layers.{i}.attention.dense.weight")
                if wq is not None and f"gpt_neox.layers.{i}.attention.query_key_value.bias" not in out:
                    out[f"gpt_neox.layers.{i}.attention.query_key_value.bias"] = torch.zeros(wq.size(0), dtype=wq.dtype)
                if wo is not None and f"gpt_neox.layers.{i}.attention.dense.bias" not in out:
                    out[f"gpt_neox.layers.{i}.attention.dense.bias"] = torch.zeros(wo.size(0), dtype=wo.dtype)

            # Save pytorch_model.bin
            torch.save(out, os.path.join(MODEL_DIR, "pytorch_model.bin"))
            print("[CONVERT] Wrote pytorch_model.bin to", MODEL_DIR)

            # infer basic config values from weights
            vocab_size = None
            hidden_size = None
            num_layers = None
            intermediate_size = None
            for k in out.keys():
                if k.endswith("embed_in.weight"):
                    vocab_size, hidden_size = out[k].shape
                    break

            layer_nums = [int(re.search(r"gpt_neox\.layers\.(\d+)\.", k).group(1))
                          for k in out if k.startswith("gpt_neox.layers.")]
            num_layers = 1 + max(layer_nums) if layer_nums else int(os.environ.get("NUM_LAYERS", 1))

            if "gpt_neox.layers.0.mlp.dense_h_to_4h.weight" in out:
                intermediate_size = out["gpt_neox.layers.0.mlp.dense_h_to_4h.weight"].shape[0]
            else:
                intermediate_size = int(os.environ.get("INTERMEDIATE_SIZE", max(4*(hidden_size or 1024)//1, (hidden_size or 1024)*4)))

            def infer_heads(hid):
                for h in [32, 16, 8, 24, 12, 4, 2]:
                    if hid % h == 0: return h
                return max(1, hid // 64) or 8

            def detect_tie(sd_map):
                try:
                    return sd_map.get("gpt_neox.embed_out.weight") is sd_map.get("gpt_neox.embed_in.weight")
                except Exception:
                    return False

            # read base cfg if present
            cfg = {}
            if BASE_CFG_JSON and os.path.exists(BASE_CFG_JSON):
                try:
                    with open(BASE_CFG_JSON) as f: cfg = json.load(f)
                except Exception:
                    cfg = {}

            num_heads = int(cfg.get("num_attention_heads") or cfg.get("num_heads") or (infer_heads(hidden_size) if hidden_size else 16))
            max_pos = int(cfg.get("max_position_embeddings") or cfg.get("max_seq_len") or 512)
            rotary_pct = float(cfg.get("rotary_pct", 0.25))
            rotary_base = float(cfg.get("rotary_emb_base", 10000.0))
            use_parallel_residual = bool(cfg.get("use_parallel_residual", False))
            layer_norm_eps = float(cfg.get("layer_norm_eps", 1e-5))
            hidden_act = str(cfg.get("hidden_act", "gelu"))
            tie_word_embeddings = bool(cfg.get("tie_word_embeddings", detect_tie(out)))

            hf_cfg = {
                "architectures": ["GPTNeoXForCausalLM"],
                "model_type": "gpt_neox",
                "vocab_size": int(vocab_size) if vocab_size else 50257,
                "hidden_size": int(hidden_size) if hidden_size else 2048,
                "intermediate_size": int(intermediate_size),
                "num_attention_heads": int(num_heads),
                "num_hidden_layers": int(num_layers),
                "max_position_embeddings": int(max_pos),
                "rotary_pct": rotary_pct,
                "rotary_emb_base": rotary_base,
                "use_parallel_residual": use_parallel_residual,
                "layer_norm_eps": layer_norm_eps,
                "hidden_act": hidden_act,
                "tie_word_embeddings": tie_word_embeddings,
            }

            with open(os.path.join(MODEL_DIR, "config.json"), "w") as f:
                json.dump(hf_cfg, f, indent=2)
            print("[CONVERT] Wrote config.json")

            # remove duplicate embed_out if tied
            if tie_word_embeddings and "gpt_neox.embed_out.weight" in out:
                out.pop("gpt_neox.embed_out.weight", None)
                torch.save(out, os.path.join(MODEL_DIR, "pytorch_model.bin"))

            # write tokenizer_config.json and special tokens
            tok_cfg = {
                "model_max_length": hf_cfg["max_position_embeddings"],
                "pad_token_id": 0, "unk_token_id": 1, "bos_token_id": 2, "eos_token_id": 3
            }
            with open(os.path.join(MODEL_DIR, "tokenizer_config.json"), "w") as f:
                json.dump(tok_cfg, f, indent=2)

            spec_map = {"pad_token": "[PAD]", "unk_token": "[UNK]", "bos_token": "[BOS]", "eos_token": "[EOS]"}
            with open(os.path.join(MODEL_DIR, "special_tokens_map.json"), "w") as f:
                json.dump(spec_map, f, indent=2)

            gen_cfg = {"max_length": hf_cfg["max_position_embeddings"], "do_sample": False, "eos_token_id": tok_cfg["eos_token_id"]}
            with open(os.path.join(MODEL_DIR, "generation_config.json"), "w") as f:
                json.dump(gen_cfg, f, indent=2)

            # copy tokenizer.json if exists
            if TOKENIZER_JSON and os.path.exists(TOKENIZER_JSON):
                shutil.copy2(TOKENIZER_JSON, os.path.join(MODEL_DIR, "tokenizer.json"))

            print("[CONVERT] Conversion complete. Model folder:", MODEL_DIR)

        if __name__ == "__main__":
            main()
        PY

        echo "[PIPELINE] Running converter..."
        python3 -u /tmp/build_hf_neox_folder_dynamic.py \
          --src-pt /tmp/final.pt \
          --tokenizer-json /tmp/tokenizer.json \
          --base-cfg-json /tmp/gptneox_config.json \
          --out-dir "$OUT_MODEL_DIR" 2>&1 | tee -a "$LOGFILE"

        ###############################################################################
        # 2) Write evaluation script (uses argparse)
        ###############################################################################
        cat >/tmp/eval_benchmarks.py <<'PY'
        #!/usr/bin/env python3
        import argparse, os, json, time, traceback, re
        import numpy as np, torch
        from datasets import load_dataset
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from lm_eval import evaluator
        from codecarbon import EmissionsTracker

        def main():
            ap = argparse.ArgumentParser()
            ap.add_argument("--model-dir", required=True, help="Hugging Face model dir to evaluate")
            ap.add_argument("--cache-dir", default="/tmp/hf_cache", help="HF cache dir")
            ap.add_argument("--tasks", default="mmlu_pro,bbh,math,ifeval,gpqa", help="Comma-separated tasks")
            ap.add_argument("--max-new-tokens", type=int, default=64, help="max_new_tokens for generation")
            ap.add_argument("--out-compact", default="/tmp/metrics_compact.json", help="compact metrics output path")
            ap.add_argument("--out-full", default="/tmp/metrics_full.json", help="full results output path")
            ap.add_argument("--out-emissions", default="/tmp/emissions.csv", help="emissions csv output path")
            args = ap.parse_args()

            MODEL_DIR = args.model_dir
            CACHE_DIR = args.cache_dir
            TASKS = [t.strip() for t in args.tasks.split(",") if t.strip()]
            OUT_COMPACT = args.out_compact
            OUT_FULL = args.out_full
            OUT_EMISSIONS = args.out_emissions

            os.makedirs(CACHE_DIR, exist_ok=True)

            PRELOAD_MAP = {
                "gpqa": [("Idavidrein/gpqa", "gpqa_main")],
            }

            def _preload_for_task(task):
                task_key = task.lower()
                entries = PRELOAD_MAP.get(task_key)
                if not entries:
                    return False
                ok = False
                for repo, split in entries:
                    try:
                        try:
                            load_dataset(repo, split, cache_dir=CACHE_DIR, token=os.environ.get("HUGGINGFACE_HUB_TOKEN"))
                        except TypeError:
                            load_dataset(repo, split, cache_dir=CACHE_DIR, use_auth_token=os.environ.get("HUGGINGFACE_HUB_TOKEN"))
                        ok = True
                        print(f"[EVAL] Preloaded {repo}::{split}")
                    except Exception as e:
                        print(f"[EVAL] Preload failed for {repo}::{split}: {e}")
                return ok

            # Start CO2 tracker
            tracker = EmissionsTracker(output_dir=os.path.dirname(OUT_EMISSIONS) or ".", output_file=os.path.basename(OUT_EMISSIONS))
            tracker.start()

            # Sanity load
            print("[EVAL] Sanity loading tokenizer + model from:", MODEL_DIR)
            tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=False)
            _ = AutoModelForCausalLM.from_pretrained(MODEL_DIR, trust_remote_code=False)

            if torch.cuda.is_available():
                print("[EVAL] CUDA available")
                print("CUDA_VISIBLE_DEVICES:", os.environ.get("CUDA_VISIBLE_DEVICES", "all"))
                for i in range(torch.cuda.device_count()):
                    try:
                        print(f"  GPU {i}:", torch.cuda.get_device_name(i))
                    except Exception:
                        print(f"  GPU {i}: <unknown>")
                eval_device = "cuda:0"
            else:
                print("[EVAL] CUDA not available - using cpu")
                eval_device = "cpu"

            def safe_json(obj):
                if isinstance(obj, dict): return {k: safe_json(v) for k,v in obj.items()}
                if isinstance(obj, (list, tuple)): return [safe_json(v) for v in obj]
                try:
                    if hasattr(obj, "item"): return obj.item()
                except Exception:
                    pass
                if isinstance(obj, np.ndarray): return obj.tolist()
                return obj

            def pick_metric(d):
                for k in ("acc_norm","acc","accuracy"):
                    if k in d and isinstance(d[k], (int,float)):
                        return k, float(d[k])
                for k,v in d.items():
                    if isinstance(v,(int,float)):
                        return k, float(v)
                return None, None

            compact = {}
            full = {}

            for task in TASKS:
                print(f"[EVAL] Running task: {task}")
                _preload_for_task(task)
                start = time.time()
                try:
                    res = evaluator.simple_evaluate(
                        model="hf",
                        model_args=f"pretrained={MODEL_DIR},tokenizer={MODEL_DIR},trust_remote_code=False",
                        tasks=[task],
                        batch_size="auto:1",
                        device=eval_device,
                        num_fewshot=0,
                        limit=None,
                    )
                    elapsed = time.time() - start
                    full[task] = safe_json(res)
                    results_dict = res.get("results", {})
                    task_result = results_dict.get(task) or next(iter(results_dict.values()), None)
                    if isinstance(task_result, dict):
                        metric_name, metric_value = pick_metric(task_result)
                        if metric_name:
                            compact[task] = {metric_name: metric_value, "time_s": round(elapsed,3)}
                            print(f"[EVAL] {task}: {metric_name}={metric_value} (time {elapsed:.1f}s)")
                        else:
                            compact[task] = {"note": "no numeric metric", "time_s": round(elapsed,3)}
                            print(f"[EVAL] {task}: no numeric metric found")
                    else:
                        compact[task] = {"note": "unexpected result structure", "time_s": round(elapsed,3)}
                        print(f"[EVAL] {task}: unexpected result structure")
                except Exception as e:
                    tb = traceback.format_exc()
                    print(f"[EVAL] Task {task} failed: {e}")
                    compact[task] = {"error": str(e)}
                    full[task] = {"error": str(e), "traceback": tb}

            emissions = tracker.stop()

            compact["co2_emissions_kg"] = float(emissions)
            full["co2_emissions_kg"] = float(emissions)

            with open(OUT_COMPACT, "w") as f:
                json.dump(safe_json(compact), f, indent=2)
            with open(OUT_FULL, "w") as f:
                json.dump(safe_json(full), f, indent=2)

            print("[EVAL] Saved compact metrics to", OUT_COMPACT)
            print("[EVAL] Saved full results to", OUT_FULL)
            print("[EVAL] Emissions written to", OUT_EMISSIONS)

        if __name__ == "__main__":
            main()
        PY

        echo "[PIPELINE] Running evaluation..."
        python3 -u /tmp/eval_benchmarks.py \
          --model-dir "$OUT_MODEL_DIR" \
          --cache-dir "$CACHE_DIR" \
          --tasks "$TASKS_ARG" \
          --max-new-tokens "$MAX_NEW_TOKENS" \
          --out-compact "$OUT_COMPACT" \
          --out-full "$OUT_FULL" \
          --out-emissions "$OUT_EMISSIONS" 2>&1 | tee -a "$LOGFILE"

        # Copy outputs to pipeline mount points (expected by pipeline system)
        cp -r "$OUT_MODEL_DIR" /tmp/hf_model_output 2>/dev/null || true
        cp "$OUT_COMPACT" /tmp/out_compact.json || true
        cp "$OUT_FULL" /tmp/out_full.json || true
        cp "$OUT_EMISSIONS" /tmp/out_emissions.csv || true
        echo "[PIPELINE] Done. Logs at $LOGFILE" | tee -a "$LOGFILE"
    args:
      - {inputValue: hf_token}
      - {inputPath: tokenizer_json}
      - {inputPath: model_pt}
      - {inputPath: model_config_json}
      - {inputPath: model_py_in}
      - {inputValue: tasks}
      - {inputValue: cache_dir}
      - {inputValue: output_model_dir}
      - {inputValue: max_new_tokens}
      - {outputPath: compact_metrics_json}
      - {outputPath: full_results_json}
      - {outputPath: emissions_csv}
      - {outputPath: logs}
      - {outputPath: converted_model}
