name: EvalBenchmarks
description: |
  Run multiple lm-eval benchmarks (gpqa, mmlu_pro, bbh, math, ifeval) on a converted
  Hugging Face model folder. Accepts converted_model (HF folder) and hf_token (string).
  Returns compact + full JSON results and an emissions CSV.

inputs:
  - name: converted_model
    type: Model
    description: Path to converted HF model folder (artifact directory)
  - name: hf_token
    type: String
    default: ""
    description: Hugging Face token (optional, if dataset is gated)
  - name: cache_dir
    type: String
    default: /tmp/hf_cache
    description: HF cache dir
  - name: tasks
    type: String
    default: "gpqa,mmlu_pro,bbh,math,ifeval"
    description: Comma-separated lm-eval task names
  - name: max_new_tokens
    type: Integer
    default: "64"
    description: Max new tokens for generation
  - name: out_compact
    type: Data
    default: /tmp/outputs/benchmark_compact.json
  - name: out_full
    type: Data
    default: /tmp/outputs/benchmark_full.json
  - name: out_emissions
    type: Data
    default: /tmp/outputs/emissions.csv

outputs:
  - name: compact_metrics
    type: Data
  - name: full_results
    type: Data
  - name: emissions_csv
    type: Data
  - name: logs
    type: Data

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v23gpu
    command:
      - sh
      - -c
      - |
        set -euo pipefail
        echo "[BOOT] Ensure lm_eval and codecarbon are installed..."
        PIP="python3 -m pip"
        PIP_DISABLE_PIP_VERSION_CHECK=1 $PIP install --quiet --no-warn-script-location lm_eval codecarbon \
          || PIP_DISABLE_PIP_VERSION_CHECK=1 $PIP install --user --quiet --no-warn-script-location lm_eval codecarbon
        export PATH="$HOME/.local/bin:$PATH"
        echo "[BOOT] Running embedded evaluator script..."
        python3 - <<'PY'
import argparse
import os
import json
import time
import traceback
import numpy as np
import torch
from huggingface_hub import HfFolder
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from lm_eval import evaluator
from codecarbon import EmissionsTracker

def parse_args():
    p = argparse.ArgumentParser(description="Evaluate HF model on multiple lm_eval tasks (no MuSR)")
    p.add_argument("--converted_model", required=True, help="Path to converted HF model folder")
    p.add_argument("--hf_token", default="", help="Hugging Face token (optional)")
    p.add_argument("--cache_dir", default="/tmp/hf_cache", help="HF cache directory")
    p.add_argument("--tasks", default="gpqa,mmlu_pro,bbh,math,ifeval", help="Comma-separated tasks")
    p.add_argument("--max_new_tokens", type=int, default=64, help="max new tokens (for generation calls)")
    p.add_argument("--out_compact", default="/tmp/outputs/benchmark_compact.json", help="compact summary output")
    p.add_argument("--out_full", default="/tmp/outputs/benchmark_full.json", help="full results output")
    p.add_argument("--out_emissions", default="/tmp/outputs/emissions.csv", help="emissions CSV output")
    return p.parse_args()

def safe_json(obj):
    if isinstance(obj, dict):
        return {k: safe_json(v) for k, v in obj.items()}
    if isinstance(obj, (list, tuple)):
        return [safe_json(v) for v in obj]
    try:
        if hasattr(obj, "item"):
            return obj.item()
    except Exception:
        pass
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    if isinstance(obj, (np.dtype, torch.dtype, torch.device, type)):
        return str(obj)
    try:
        json.dumps(obj)
        return obj
    except Exception:
        return str(obj)

def pick_metric(d):
    for k in ("acc_norm", "acc", "accuracy"):
        if k in d and isinstance(d[k], (int, float)):
            return k, float(d[k])
    for k, v in d.items():
        if isinstance(v, (int, float)):
            return k, float(v)
    return None, None

def preload_for_task(task, cache_dir, token):
    PRELOAD_MAP = {"gpqa": [("Idavidrein/gpqa", "gpqa_main")]}
    task_key = task.lower()
    entries = PRELOAD_MAP.get(task_key)
    if not entries:
        return False
    ok_any = False
    for repo, split in entries:
        print("[PRELOAD] Loading dataset %s split=%s ..." % (repo, split))
        try:
            try:
                load_dataset(repo, split, cache_dir=cache_dir, token=token)
            except TypeError:
                load_dataset(repo, split, cache_dir=cache_dir, use_auth_token=token)
            print("[PRELOAD] OK %s::%s" % (repo, split))
            ok_any = True
        except Exception as e:
            print("[PRELOAD] Failed %s::%s: %s" % (repo, split, str(e)))
    return ok_any

def main():
    args = parse_args()
    CONVERTED = os.path.abspath(args.converted_model)
    CACHE = os.path.abspath(args.cache_dir)
    OUT_COMPACT = os.path.abspath(args.out_compact)
    OUT_FULL = os.path.abspath(args.out_full)
    OUT_EMISSIONS = os.path.abspath(args.out_emissions)
    for p in (os.path.dirname(OUT_COMPACT), os.path.dirname(OUT_FULL), os.path.dirname(OUT_EMISSIONS), CACHE):
        if p:
            os.makedirs(p, exist_ok=True)

    TOKEN = args.hf_token or ""
    if TOKEN:
        os.environ["HUGGINGFACE_HUB_TOKEN"] = TOKEN
        os.environ["HF_TOKEN"] = TOKEN
        HfFolder.save_token(TOKEN)
        print("[EVAL] saved HF token")

    os.environ.update({
        "HF_HUB_DISABLE_TELEMETRY": "1",
        "HF_HOME": CACHE,
        "HF_DATASETS_CACHE": CACHE,
        "HUGGINGFACE_HUB_CACHE": CACHE
    })

    TASKS = [t.strip() for t in args.tasks.split(",") if t.strip()]
    COMPOSITE_GROUP = ["mmlu_pro", "bbh", "math", "ifeval"]

    emissions_out_dir = os.path.dirname(OUT_EMISSIONS) or "."
    tracker = EmissionsTracker(output_dir=emissions_out_dir, output_file=os.path.basename(OUT_EMISSIONS))
    tracker.start()

    print("[EVAL] Loading tokenizer + model from: %s" % CONVERTED)
    tokenizer = AutoTokenizer.from_pretrained(CONVERTED, trust_remote_code=False)
    _ = AutoModelForCausalLM.from_pretrained(CONVERTED, trust_remote_code=False)

    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print("[EVAL] CUDA visible devices: %s" % os.environ.get("CUDA_VISIBLE_DEVICES", "all"))
        print("[EVAL] torch reports %d CUDA device(s)." % gpu_count)
        for i in range(gpu_count):
            try:
                name = torch.cuda.get_device_name(i)
            except Exception:
                name = "<unknown>"
            try:
                cap = torch.cuda.get_device_capability(i)
            except Exception:
                cap = ("?", "?")
            print("  GPU %d: %s  |  capability: %s" % (i, name, str(cap)))
        eval_device = "cuda:0"
    else:
        print("[EVAL] CUDA not available -- using CPU")
        eval_device = "cpu"
    print("[EVAL] Using device string for lm_eval: %s" % eval_device)

    compact_results = {}
    full_results = {}

    for task in TASKS:
        print("\n[EVAL] Running task: %s" % task)
        preload_for_task(task, CACHE, TOKEN)
        start_t = time.time()
        try:
            model_args_str = "pretrained=%s,tokenizer=%s,trust_remote_code=False" % (CONVERTED, CONVERTED)
            res = evaluator.simple_evaluate(
                model="hf",
                model_args=model_args_str,
                tasks=[task],
                batch_size="auto:1",
                device=eval_device,
                num_fewshot=0,
                limit=None,
            )
            dur = time.time() - start_t
            full_results[task] = safe_json(res)

            results_dict = res.get("results", {})
            task_result = results_dict.get(task) or (next(iter(results_dict.values())) if results_dict else None)

            if isinstance(task_result, dict):
                metric_name, metric_value = pick_metric(task_result)
                if metric_name is not None:
                    compact_results[task] = {metric_name: metric_value, "time_s": round(dur, 3)}
                    print("[EVAL] %s: %s = %s  (time %.1f s)" % (task, metric_name, str(metric_value), dur))
                else:
                    compact_results[task] = {"note": "no numeric metric found", "time_s": round(dur, 3)}
                    print("[EVAL] %s: no numeric metric found  (time %.1f s)" % (task, dur))
            else:
                compact_results[task] = {"note": "unexpected result structure", "raw": safe_json(task_result), "time_s": round(dur,3)}
                print("[EVAL] %s: unexpected result structure  (time %.1f s)" % (task, dur))

        except Exception as e:
            dur = time.time() - start_t
            tb = traceback.format_exc()
            print("[EVAL] Task " + task + " failed with exception:\\n" + tb)
            compact_results[task] = {"error": str(e), "time_s": round(dur, 3)}
            full_results[task] = {"error": str(e), "traceback": tb, "time_s": round(dur, 3)}

    emissions = tracker.stop()
    compact_results["co2_emissions_kg"] = float(emissions)
    full_results["co2_emissions_kg"] = float(emissions)

    found_vals = []
    for t in COMPOSITE_GROUP:
        entry = compact_results.get(t)
        if isinstance(entry, dict):
            for k, v in entry.items():
                if k == "time_s" or k.startswith("note") or k == "error":
                    continue
                if isinstance(v, (int, float)):
                    found_vals.append(float(v))
                    break
    if found_vals:
        composite_score = float(np.mean(found_vals))
        compact_results["composite_mean"] = round(composite_score, 6)
        print("\n[EVAL] Composite mean over %s: %.6f" % (str(COMPOSITE_GROUP), composite_score))
    else:
        print("\n[EVAL] No numeric values found for composite computation.")

    with open(OUT_COMPACT, "w") as f:
        json.dump(safe_json(compact_results), f, indent=2)
    with open(OUT_FULL, "w") as f:
        json.dump(safe_json(full_results), f, indent=2)

    print("[EVAL] Saved compact summary to: %s" % OUT_COMPACT)
    print("[EVAL] Saved full results to: %s" % OUT_FULL)
    print("[EVAL] Emissions CSV at: %s" % OUT_EMISSIONS)

if __name__ == "__main__":
    main()
PY
    args:
      - --converted_model
      - {inputPath: converted_model}
      - --hf_token
      - {inputValue: hf_token}
      - --cache_dir
      - {inputValue: cache_dir}
      - --tasks
      - {inputValue: tasks}
      - --max_new_tokens
      - {inputValue: max_new_tokens}
      - --out_compact
      - {inputPath: out_compact}
      - --out_full
      - {inputPath: out_full}
      - --out_emissions
      - {inputPath: out_emissions}
