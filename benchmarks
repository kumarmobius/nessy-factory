name: HFConvertor
description: |
  Convert a Gemma3 / GPT-NeoX checkpoint + tokenizer/config into a Hugging Face model folder.
  This component ONLY performs conversion and returns the HF folder as the single artifact.

inputs:
  - name: tokenizer_json
    type: Model
  - name: model_pt
    type: Model
  - name: model_config_json
    type: Data
  - name: hf_token
    type: String
    default: ""
  - name: cache_dir
    type: String
    default: "/tmp/hf_cache"
  - name: output_model_dir
    type: String
    default: "/tmp/hf_model"
  - name: max_new_tokens
    type: Integer
    default: "80"

outputs:
  - name: converted_model
    type: Model

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v23gpu
    command:
      - bash
      - -eu
      - -c
      - |
        set -o pipefail

        # --- positional args forwarded from container args ---
        HF_TOKEN_ARG="${1:-}"
        TOKENIZER_PATH="${2:-}"
        PT_PATH="${3:-}"
        CFG_JSON_PATH="${4:-}"
        CACHE_DIR="${5:-/tmp/hf_cache}"
        OUT_MODEL_DIR="${6:-/tmp/hf_model}"
        MAX_NEW_TOKENS="${7:-80}"
        OUT_CONVERTED="${8:-/tmp/outputs/converted_model/data}"

        echo "[CONVERT] args: HF_TOKEN='${HF_TOKEN_ARG}' TOKENIZER='${TOKENIZER_PATH}' PT='${PT_PATH}' CFG='${CFG_JSON_PATH}' CACHE_DIR='${CACHE_DIR}' OUT_MODEL_DIR='${OUT_MODEL_DIR}' OUT_CONVERTED='${OUT_CONVERTED}'"

        mkdir -p "$(dirname "$OUT_CONVERTED")" "$OUT_CONVERTED" "$OUT_MODEL_DIR" "$CACHE_DIR" 2>/dev/null || true

        # export HF token if provided
        if [ -n "$HF_TOKEN_ARG" ]; then
          export HUGGINGFACE_HUB_TOKEN="$HF_TOKEN_ARG"
          export HF_TOKEN="$HF_TOKEN_ARG"
          echo "[CONVERT] Saved HF token"
        fi

        # Copy inputs into predictable paths inside container (if present)
        if [ -n "$TOKENIZER_PATH" ] && [ -f "$TOKENIZER_PATH" ]; then
          cp "$TOKENIZER_PATH" /tmp/tokenizer.json
          echo "[CONVERT] Copied tokenizer.json -> /tmp/tokenizer.json"
        else
          echo "[CONVERT] No tokenizer.json path provided or file missing."
        fi

        if [ -n "$PT_PATH" ] && [ -f "$PT_PATH" ]; then
          cp "$PT_PATH" /tmp/final.pt
          echo "[CONVERT] Copied model_pt -> /tmp/final.pt"
        else
          echo "[CONVERT][WARN] model_pt not provided or missing."
        fi

        if [ -n "$CFG_JSON_PATH" ] && [ -f "$CFG_JSON_PATH" ]; then
          cp "$CFG_JSON_PATH" /tmp/gptneox_config.json
          echo "[CONVERT] Copied model_config_json -> /tmp/gptneox_config.json"
        fi

        # Running the Python conversion script
        echo "[CONVERT] Running converter script..."

        python3 - <<'PYTHON_SCRIPT'
        import os, json, torch
        from collections import OrderedDict

        # --- File paths ---
        SRC_PT = "/tmp/final.pt"
        TOKENIZER_JSON = "/tmp/tokenizer.json"
        BASE_CFG_JSON = "/tmp/gptneox_config.json"
        MODEL_DIR = "/tmp/hf_model"

        os.makedirs(MODEL_DIR, exist_ok=True)

        # Load state dict
        sd = torch.load(SRC_PT, map_location="cpu")
        if isinstance(sd, dict) and "state_dict" in sd:
            sd = sd["state_dict"]
        out = OrderedDict(sd)

        # Ensure compatibility (renaming layernorm, handling embeddings)
        if "gpt_neox.final_layernorm.weight" in out and "gpt_neox.final_layer_norm.weight" not in out:
            out["gpt_neox.final_layer_norm.weight"] = out.pop("gpt_neox.final_layernorm.weight")
            out["gpt_neox.final_layer_norm.bias"] = out.pop("gpt_neox.final_layernorm.bias")

        if "gpt_neox.embed_out.weight" not in out and "gpt_neox.embed_in.weight" in out:
            out["gpt_neox.embed_out.weight"] = out["gpt_neox.embed_in.weight"]

        # Save pytorch model
        torch.save(out, os.path.join(MODEL_DIR, "pytorch_model.bin"))
        print("[CONVERTER] Wrote pytorch_model.bin")

        # Load vocabulary size and hidden size
        vocab_size, hidden_size = out["gpt_neox.embed_in.weight"].shape

        # --- Model configuration ---
        hf_cfg = {
            "architectures": ["GPTNeoXForCausalLM"],
            "model_type": "gpt_neox",
            "vocab_size": vocab_size,
            "hidden_size": hidden_size,
            "num_attention_heads": 16,
            "num_hidden_layers": 1,  # Adjust based on actual layer count
            "max_position_embeddings": 512,
            "rotary_pct": 0.25,
            "rotary_emb_base": 10000.0,
            "use_parallel_residual": False,
            "layer_norm_eps": 1e-5,
            "hidden_act": "gelu",
            "tie_word_embeddings": True,
        }

        with open(os.path.join(MODEL_DIR, "config.json"), "w") as f:
            json.dump(hf_cfg, f, indent=2)

        print("[CONVERTER] Wrote config.json")

        # Tokenizer configuration (if available)
        if os.path.exists(TOKENIZER_JSON):
            shutil.copy2(TOKENIZER_JSON, os.path.join(MODEL_DIR, "tokenizer.json"))
            print("[CONVERTER] Copied tokenizer.json to model directory.")

        # Special tokens map
        spec_map = {
            "pad_token": "[PAD]", "unk_token": "[UNK]", "bos_token": "[BOS]", "eos_token": "[EOS]"
        }
        with open(os.path.join(MODEL_DIR, "special_tokens_map.json"), "w") as f:
            json.dump(spec_map, f, indent=2)

        # Generation config
        gen_cfg = {"max_length": 512, "do_sample": False, "eos_token_id": 3}
        with open(os.path.join(MODEL_DIR, "generation_config.json"), "w") as f:
            json.dump(gen_cfg, f, indent=2)

        print("[CONVERTER] Conversion completed.")
        PYTHON_SCRIPT

        # Ensure converted artifact path exists and copy HF folder into it
        mkdir -p "$(dirname "$OUT_CONVERTED")" "$OUT_CONVERTED"
        if [ -d "$OUT_MODEL_DIR" ]; then
          cp -r "$OUT_MODEL_DIR"/. "$OUT_CONVERTED"/
          echo "[CONVERT] Copied HF folder into artifact path: $OUT_CONVERTED"
        else
          echo "[CONVERT][ERROR] Expected HF model dir not found at $OUT_MODEL_DIR."
        fi

        echo "[CONVERT] Done."
    args:
      - inputValue: hf_token
      - inputPath: tokenizer_json
      - inputPath: model_pt
      - inputPath: model_config_json
      - inputValue: cache_dir
      - inputValue: output_model_dir
      - inputValue: max_new_tokens
      - outputPath: converted_model
