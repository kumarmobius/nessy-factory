name: hf_convertor
description: |
  Convert a Gemma3 / GPT-NeoX checkpoint + tokenizer/config into a Hugging Face model folder.
  This component ONLY performs conversion and returns the HF folder as the single artifact.

inputs:
  - name: tokenizer_json
    type: Model
  - name: model_pt
    type: Model
  - name: model_config_json
    type: Data
  - name: cache_dir
    type: String
    default: "/tmp/hf_cache"
  - name: output_model_dir
    type: String
    default: "/tmp/hf_model"
  - name: max_new_tokens
    type: Integer
    default: "80"

outputs:
  - name: converted_model
    type: Model

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v23gpu
    command:
      - bash
      - -eu
      - -c
      - |
        set -o pipefail

        # Pass arguments directly to the Python script
        TOKENIZER_PATH="${TOKENIZER_JSON}"
        PT_PATH="${MODEL_PT}"
        CFG_JSON_PATH="${MODEL_CONFIG_JSON}"
        CACHE_DIR="${CACHE_DIR:-/tmp/hf_cache}"
        OUT_MODEL_DIR="${OUTPUT_MODEL_DIR:-/tmp/hf_model}"
        MAX_NEW_TOKENS="${MAX_NEW_TOKENS:-80}"
        OUT_CONVERTED="${CONVERTED_MODEL:-/tmp/outputs/converted_model/data}"

        # Create the output directory if it doesn't exist
        mkdir -p "$OUT_CONVERTED"

        # Running the Python conversion script directly
        python3 - <<'PYTHON_SCRIPT'
        import os, json, torch
        from collections import OrderedDict

        # --- File paths ---
        SRC_PT = os.path.join(os.getcwd(), os.getenv("PT_PATH"))  # Get the path directly from the environment variable
        TOKENIZER_JSON = os.path.join(os.getcwd(), os.getenv("TOKENIZER_PATH"))  # Get the path directly from the environment variable
        BASE_CFG_JSON = os.path.join(os.getcwd(), os.getenv("CFG_JSON_PATH"))  # Get the path directly from the environment variable
        MODEL_DIR = "/tmp/hf_model"

        os.makedirs(MODEL_DIR, exist_ok=True)

        # Load the provided configuration from the model_config_json
        if os.path.exists(BASE_CFG_JSON):
            with open(BASE_CFG_JSON, "r") as f:
                hf_cfg = json.load(f)
        else:
            raise ValueError(f"Configuration file {BASE_CFG_JSON} not found!")

        print("[CONVERTER] Loaded model configuration from", BASE_CFG_JSON)

        # Load state dict
        sd = torch.load(SRC_PT, map_location="cpu")
        if isinstance(sd, dict) and "state_dict" in sd:
            sd = sd["state_dict"]
        out = OrderedDict(sd)

        # Ensure compatibility (renaming layernorm, handling embeddings)
        if "gpt_neox.final_layernorm.weight" in out and "gpt_neox.final_layer_norm.weight" not in out:
            out["gpt_neox.final_layer_norm.weight"] = out.pop("gpt_neox.final_layernorm.weight")
            out["gpt_neox.final_layer_norm.bias"] = out.pop("gpt_neox.final_layernorm.bias")

        if "gpt_neox.embed_out.weight" not in out and "gpt_neox.embed_in.weight" in out:
            out["gpt_neox.embed_out.weight"] = out["gpt_neox.embed_in.weight"]

        # Save pytorch model
        torch.save(out, os.path.join(MODEL_DIR, "pytorch_model.bin"))
        print("[CONVERTER] Wrote pytorch_model.bin")

        # Tokenizer configuration (if available)
        if os.path.exists(TOKENIZER_JSON):
            shutil.copy2(TOKENIZER_JSON, os.path.join(MODEL_DIR, "tokenizer.json"))
            print("[CONVERTER] Copied tokenizer.json to model directory.")

        # Special tokens map
        spec_map = {
            "pad_token": "[PAD]", "unk_token": "[UNK]", "bos_token": "[BOS]", "eos_token": "[EOS]"
        }
        with open(os.path.join(MODEL_DIR, "special_tokens_map.json"), "w") as f:
            json.dump(spec_map, f, indent=2)

        # Generation config
        gen_cfg = {"max_length": hf_cfg.get("max_position_embeddings", 512), "do_sample": False, "eos_token_id": hf_cfg.get("eos_token_id", 3)}
        with open(os.path.join(MODEL_DIR, "generation_config.json"), "w") as f:
            json.dump(gen_cfg, f, indent=2)

        print("[CONVERTER] Conversion completed.")
        PYTHON_SCRIPT

        # Ensure converted artifact path exists and copy HF folder into it
        mkdir -p "$(dirname "$OUT_CONVERTED")" "$OUT_CONVERTED"
        if [ -d "$OUT_MODEL_DIR" ]; then
          cp -r "$OUT_MODEL_DIR"/. "$OUT_CONVERTED"/
          echo "[CONVERT] Copied HF folder into artifact path: $OUT_CONVERTED"
        else
          echo "[CONVERT][ERROR] Expected HF model dir not found at $OUT_MODEL_DIR."
        fi

        echo "[CONVERT] Done."
    args:
      - inputPath: tokenizer_json
      - inputPath: model_pt
      - inputPath: model_config_json
      - inputValue: cache_dir
      - inputValue: output_model_dir
      - inputValue: max_new_tokens
      - outputPath: converted_model
