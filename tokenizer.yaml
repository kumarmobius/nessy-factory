name: Train Byte-Level BPE Tokenizer (NESY v21)
description: Trains a byte-level BPE tokenizer on a newline-delimited text corpus using nesy-factory’s ByteLevelBPETokenizer.

inputs:
  - name: train_corpus
    type: Data
  - name: vocab_size
    type: Integer
    default: "32000"
  - name: min_frequency
    type: Integer
    default: "2"
  - name: add_bos_eos
    type: String
    default: "true"          # "true"/"false" (parsed inside the script)
  - name: special_tokens
    type: String
    default: "[PAD],[UNK],[BOS],[EOS]"

outputs:
  - name: tokenizer_json
    type: Model
  - name: training_report
    type: Data
  - name: schema_json
    type: Data

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v21
    command:
      - bash
      - -eu
      - -c
      - |-
        set -o pipefail

        # write a tiny wrapper that calls nesy-factory's ByteLevelBPETokenizer
        cat >/tmp/train_tokenizer_nesy_v21.py <<'PY'
        import argparse, json, os, sys

        # Try both module paths (some builds export it under tokenize.py, others as tokenizer.py)
        TokenizerClass = None
        import_errors = {}
        try:
            from nesy_factory.language_model.tokenize import ByteLevelBPETokenizer as _Tok
            TokenizerClass = _Tok
        except Exception as e:
            import_errors["nesy_factory.language_model.tokenize"] = str(e)
            try:
                from nesy_factory.language_model.tokenizer import ByteLevelBPETokenizer as _Tok2
                TokenizerClass = _Tok2
            except Exception as e2:
                import_errors["nesy_factory.language_model.tokenizer"] = str(e2)

        def parse_bool(v: str) -> bool:
            return str(v).strip().lower() in ("true", "1", "yes", "y")

        def build_flat_schema(a, specials_parsed, add_bos_eos_resolved, actual_vocab_size=None):
            schema = {
                "train_corpus": a.train_corpus,
                "vocab_size": a.vocab_size,
                "min_frequency": a.min_frequency,
                "add_bos_eos": a.add_bos_eos,          # raw input string
                "special_tokens": a.special_tokens,

                # effective config (flat)
                "model_type": "BPE",
                "pre_tokenizer": "ByteLevel",
                "decoder": "ByteLevel",
                "unk_token": "[UNK]",
                "special_tokens_parsed": specials_parsed,
                "add_bos_eos_resolved": add_bos_eos_resolved,
                "target_vocab_size": a.vocab_size,
            }
            if actual_vocab_size is not None:
                schema["actual_vocab_size"] = actual_vocab_size
            return schema

        def main():
            ap = argparse.ArgumentParser()
            ap.add_argument("--train-corpus", required=True)
            ap.add_argument("--vocab-size", type=int, default=32000)
            ap.add_argument("--min-frequency", type=int, default=2)
            ap.add_argument("--special-tokens", default="[PAD],[UNK],[BOS],[EOS]")
            ap.add_argument("--add-bos-eos", default="true")
            ap.add_argument("--tokenizer-json", required=True)
            ap.add_argument("--training-report", required=True)
            ap.add_argument("--schema-output", required=True)
            a = ap.parse_args()

            # Fail fast if class couldn't be imported
            if TokenizerClass is None:
                err = {
                    "status": "error",
                    "error": "ByteLevelBPETokenizer could not be imported from nesy_factory.",
                    "import_errors": import_errors,
                }
                os.makedirs(os.path.dirname(a.training_report) or ".", exist_ok=True)
                with open(a.training_report, "w", encoding="utf-8") as f:
                    json.dump(err, f, indent=2)
                print("[FATAL] ByteLevelBPETokenizer import failed. See training_report for details.", file=sys.stderr)
                sys.exit(1)

            specials = [s.strip() for s in (a.special_tokens or "").split(",") if s.strip()]
            add_bos_eos_flag = parse_bool(a.add_bos_eos)

            print(f"[INFO] Training tokenizer (nesy-factory) on {a.train_corpus}")
            print(f"[INFO] Target vocab size: {a.vocab_size}")
            print(f"[INFO] Min frequency: {a.min_frequency}")
            print(f"[INFO] Special tokens: {specials}")
            print(f"[INFO] Add BOS/EOS: {add_bos_eos_flag}")

            tok = TokenizerClass()
            report = tok.run(
                text_file=a.train_corpus,
                vocab_size=a.vocab_size,
                min_frequency=a.min_frequency,
                special_tokens=a.special_tokens,
                add_bos_eos=add_bos_eos_flag,
            )

            # Save tokenizer JSON through the class API
            os.makedirs(os.path.dirname(a.tokenizer_json) or ".", exist_ok=True)
            tok.save(a.tokenizer_json)

            # Save training report (dict returned by tok.run)
            os.makedirs(os.path.dirname(a.training_report) or ".", exist_ok=True)
            with open(a.training_report, "w", encoding="utf-8") as f:
                json.dump(report, f, indent=2)

            # Save a flat schema JSON to mirror your earlier brick’s output shape
            schema_flat = build_flat_schema(
                a,
                specials_parsed=specials,
                add_bos_eos_resolved=add_bos_eos_flag,
                actual_vocab_size=report.get("actual_vocab_size"),
            )
            os.makedirs(os.path.dirname(a.schema_output) or ".", exist_ok=True)
            with open(a.schema_output, "w", encoding="utf-8") as f:
                json.dump(schema_flat, f, indent=2)

            print(f"[SUCCESS] Tokenizer saved to {a.tokenizer_json}")
            print(f"[SUCCESS] Training report saved to {a.training_report}")
            print(f"[SUCCESS] Schema JSON saved to {a.schema_output}")
            print(f"[INFO] Final vocab size: {report.get('actual_vocab_size')}")
        
        if __name__ == "__main__":
            main()
        PY

        # run the helper with all CLI args the brick passes
        python3 -u /tmp/train_tokenizer_nesy_v21.py "$@"
    args:
      - --train-corpus
      - {inputPath: train_corpus}
      - --vocab-size
      - {inputValue: vocab_size}
      - --min-frequency
      - {inputValue: min_frequency}
      - --special-tokens
      - {inputValue: special_tokens}
      - --add-bos-eos
      - {inputValue: add_bos_eos}
      - --tokenizer-json
      - {outputPath: tokenizer_json}
      - --training-report
      - {outputPath: training_report}
      - --schema-output
      - {outputPath: schema_json}
