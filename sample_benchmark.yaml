name: EvalBenchmarks
description: Run a small lm-eval snippet against a local HF-style model folder
inputs:
  - name: converted_model
    type: Model
  - name: cache_dir
    type: String
    default: /tmp/hf_cache
  - name: tasks
    type: String
    default: ifeval
outputs:
  - name: compact_metrics
    type: Data
  - name: full_results
    type: Data
implementation:
  container:
    image: kumar2004/latest:v4
    resources:
      requests:
        cpu: "2"
        memory: "8Gi"
        ephemeral-storage: "5Gi"
      limits:
        cpu: "4"
        memory: "16Gi"
        ephemeral-storage: "10Gi"
    command:
      - python3
      - -u
      - -c
      - |
        import sys, os, json, traceback
        import numpy as np
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from lm_eval import evaluator
        TaskManager = None
        try:
            from lm_eval.tasks import TaskManager
        except:
            pass
        
        def eprint(*a, **k):
            print(*a, file=sys.stderr, **k)
            sys.stderr.flush()
        
        def safe_json(obj):
            if isinstance(obj, dict):
                return {k: safe_json(v) for k, v in obj.items()}
            if isinstance(obj, (list, tuple)):
                return [safe_json(v) for v in obj]
            try:
                if hasattr(obj, "item"):
                    return obj.item()
            except:
                pass
            try:
                import torch as _t
                if isinstance(obj, (np.ndarray, _t.Tensor)):
                    return obj.tolist()
            except:
                pass
            if isinstance(obj, (np.dtype, torch.dtype, torch.device, type)):
                return str(obj)
            return obj
        
        CKPT = os.path.abspath(sys.argv[1])
        CACHE = os.path.abspath(sys.argv[2])
        TASKS = [t.strip() for t in sys.argv[3].split(",") if t.strip()]
        OUT_COMPACT = os.path.abspath(sys.argv[4])
        OUT_FULL = os.path.abspath(sys.argv[5])
        
        os.makedirs(CACHE, exist_ok=True)
        for p in [os.path.dirname(OUT_COMPACT), os.path.dirname(OUT_FULL)]:
            if p:
                os.makedirs(p, exist_ok=True)
        
        os.environ.update({
            "HF_HUB_DISABLE_TELEMETRY": "1",
            "HF_HUB_ENABLE_HF_TRANSFER": "0",
            "HF_HOME": CACHE,
            "HF_DATASETS_CACHE": CACHE,
            "HUGGINGFACE_HUB_CACHE": CACHE
        })
        
        try:
            eprint("Loading model from:", CKPT)
            tok = AutoTokenizer.from_pretrained(CKPT, trust_remote_code=False)
            # model = AutoModelForCausalLM.from_pretrained(CKPT, trust_remote_code=False)
            eprint("Model loaded successfully")
        except Exception as exc:
            eprint("FATAL load error:", exc)
            eprint(traceback.format_exc())
            err_data = {"error": str(exc), "traceback": traceback.format_exc()}
            try:
                with open(OUT_COMPACT, "w") as f:
                    json.dump(err_data, f)
            except:
                pass
            try:
                with open(OUT_FULL, "w") as f:
                    json.dump(err_data, f)
            except:
                pass
            sys.exit(1)
        
        if TaskManager is not None:
            try:
                tm = TaskManager()
                bbh_tasks = [k for k in tm.task_index if k.startswith("bbh_zeroshot_")]
                eprint("BBH tasks detected:", bbh_tasks)
            except Exception as exc:
                eprint("BBH detection failed:", exc)
        
        if not TASKS:
            TASKS = ["ifeval"]
        eprint("Evaluating tasks:", TASKS)

        if torch.cuda.is_available():
            eprint(f"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        else:
            eprint("No GPU available, using CPU")
            import psutil
            eprint(f"Available system RAM: {psutil.virtual_memory().total / 1e9:.2f} GB")
            eprint(f"Available system RAM (free): {psutil.virtual_memory().available / 1e9:.2f} GB")

        
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        model_args = "pretrained={},tokenizer={},trust_remote_code=False,low_cpu_mem_usage=True".format(CKPT, CKPT)
        # model_args = "pretrained=" + CKPT + ",tokenizer=" + CKPT + ",trust_remote_code=False"
        # model_args = "pretrained={},tokenizer={},trust_remote_code=False,device_map=auto,torch_dtype=float16,low_cpu_mem_usage=True".format(CKPT, CKPT)

        if torch.cuda.is_available():
            eprint(f"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        else:
            eprint("No GPU available, using CPU")
            import psutil
            eprint(f"Available system RAM: {psutil.virtual_memory().total / 1e9:.2f} GB")
            eprint(f"Available system RAM (free): {psutil.virtual_memory().available / 1e9:.2f} GB")

        
        try:
            results = evaluator.simple_evaluate(
                model="hf",
                model_args=model_args,
                tasks=TASKS,
                batch_size="auto:1",
                device=device,
                num_fewshot=0,
                limit=None
            )
        except Exception as exc:
            eprint("FATAL eval error:", exc)
            eprint(traceback.format_exc())
            err_data = {"error": str(exc), "traceback": traceback.format_exc()}
            try:
                with open(OUT_COMPACT, "w") as f:
                    json.dump(err_data, f)
            except:
                pass
            try:
                with open(OUT_FULL, "w") as f:
                    json.dump(err_data, f)
            except:
                pass
            sys.exit(1)
        
        try:
            with open(OUT_FULL, "w") as f:
                json.dump(safe_json(results), f, indent=2)
            eprint("Wrote full results to:", OUT_FULL)
        except Exception as exc:
            eprint("Failed writing full results:", exc)
        
        summary = {}
        for k, v in results.get("results", {}).items():
            if isinstance(v, dict):
                acc = v.get("acc", v.get("acc,none", None))
                if acc is None:
                    for kk, vv in v.items():
                        if isinstance(vv, (int, float)):
                            acc = vv
                            break
                summary[k] = {"acc": acc}
            else:
                summary[k] = {"acc": None}
        
        try:
            with open(OUT_COMPACT, "w") as f:
                json.dump(safe_json(summary), f, indent=2)
            eprint("Wrote compact summary to:", OUT_COMPACT)
        except Exception as exc:
            eprint("Failed writing compact summary:", exc)
    args:
      - inputPath: converted_model
      - inputValue: cache_dir
      - inputValue: tasks
      - outputPath: compact_metrics
      - outputPath: full_results
