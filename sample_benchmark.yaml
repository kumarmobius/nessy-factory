name: EvalBBHDetected
description: >
  Run a small lm-eval snippet (BBH detection + tasks) against a local HF-style model folder.
inputs:
  - name: converted_model
    type: Model
  - name: cache_dir
    type: String
    default: /tmp/hf_cache
  - name: tasks
    type: String
    default: ifeval
outputs:
  - name: results
    type: Data
implementation:
  container:
    image: kumar2004/latest:v2
    command:
      - bash
      - -lc
      - |
        cat > /tmp/eval_bbh_detected.py <<'PY'
        #!/usr/bin/env python3
        import sys, os, json, time, traceback
        import numpy as np
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from lm_eval import evaluator
        try:
            from lm_eval.tasks import TaskManager
        except Exception:
            TaskManager = None

        def eprint(*a, **k):
            print(*a, file=sys.stderr, **k); sys.stderr.flush()

        def safe_json(obj):
            if isinstance(obj, dict):
                return {k: safe_json(v) for k, v in obj.items()}
            if isinstance(obj, (list, tuple)):
                return [safe_json(v) for v in obj]
            try:
                if hasattr(obj, "item"):
                    return obj.item()
            except Exception:
                pass
            try:
                import torch as _t
                if isinstance(obj, (np.ndarray, _t.Tensor)):
                    return obj.tolist()
            except Exception:
                pass
            if isinstance(obj, (np.dtype, torch.dtype, torch.device, type)):
                return str(obj)
            return obj

        def main():
            import argparse
            parser = argparse.ArgumentParser()
            parser.add_argument("--converted_model", required=True)
            parser.add_argument("--cache_dir", default="/tmp/hf_cache")
            parser.add_argument("--tasks", default="ifeval")
            args = parser.parse_args()

            CKPT = os.path.abspath(args.converted_model)
            CACHE = os.path.abspath(args.cache_dir)
            TASKS = [t.strip() for t in args.tasks.split(",") if t.strip()]

            os.makedirs(CACHE, exist_ok=True)

            os.environ.update({
                "HF_HUB_DISABLE_TELEMETRY": "1",
                "HF_HUB_ENABLE_HF_TRANSFER": "0",
                "HF_HOME": CACHE,
                "HF_DATASETS_CACHE": CACHE,
                "HUGGINGFACE_HUB_CACHE": CACHE
            })

            try:
                eprint("Loading tokenizer + model from:", CKPT)
                tok = AutoTokenizer.from_pretrained(CKPT, trust_remote_code=False)
                _ = AutoModelForCausalLM.from_pretrained(CKPT, trust_remote_code=False)
                eprint("Loaded model and tokenizer successfully")
            except Exception as exc:
                eprint("FATAL: failed to load model/tokenizer:", exc)
                eprint(traceback.format_exc())
                # try to write an error artifact next to the model path
                try:
                    with open(os.path.join(CKPT, "result_dynamic_error.txt"), "w") as ef:
                        ef.write(str(exc) + "\n\n")
                        ef.write(traceback.format_exc())
                except Exception:
                    pass
                sys.exit(1)

            # optional detection of BBH tasks
            if TaskManager is not None:
                try:
                    tm = TaskManager()
                    bbh_tasks = [k for k in tm.task_index if k.startswith("bbh_zeroshot_")]
                    eprint("Detected BBH tasks:", bbh_tasks)
                except Exception as exc:
                    eprint("BBH detection failed:", exc)


            eprint("Evaluating tasks:", TASKS)

            device = "cuda:0" if torch.cuda.is_available() else "cpu"
            model_args_str = "pretrained=" + CKPT + ",tokenizer=" + CKPT + ",trust_remote_code=False"

            try:
                results = evaluator.simple_evaluate(
                    model="hf",
                    model_args=model_args_str,
                    tasks=TASKS,
                    batch_size="auto:1",
                    device=device,
                    num_fewshot=0,
                    limit=None
                )
            except Exception as exc:
                eprint("FATAL: evaluator.simple_evaluate failed:", exc)
                eprint(traceback.format_exc())
                try:
                    with open(os.path.join(CKPT, "result_dynamic_error.txt"), "w") as ef:
                        ef.write(str(exc) + "\n\n")
                        ef.write(traceback.format_exc())
                except Exception:
                    pass
                sys.exit(1)

            out_path = os.path.join(CKPT, "result_dynamic.json")
            with open(out_path, "w") as f:
                json.dump(safe_json(results), f, indent=2)
            eprint("Saved results to:", out_path)

            # small summary artifact
            summary = {}
            for k, v in results.get("results", {}).items():
                if isinstance(v, dict):
                    acc = v.get("acc", v.get("acc,none", None))
                    summary[k] = {"acc": acc}
            with open(os.path.join(CKPT, "result_summary.json"), "w") as f:
                json.dump(summary, f, indent=2)
            eprint("Saved summary to:", os.path.join(CKPT, "result_summary.json"))

        if __name__ == "__main__":
            try:
                main()
            except Exception as e:
                eprint("UNHANDLED EXCEPTION:", e)
                eprint(traceback.format_exc())
                sys.exit(1)
        PY
        exec python3 -u /tmp/eval_bbh_detected.py "$0" "$@"
    args:
      - --converted_model
      - {inputPath: converted_model}
      - --cache_dir
      - {inputValue: cache_dir}
      - --tasks
      - {inputValue: tasks}
