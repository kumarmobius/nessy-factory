name: Run Inference
description: |
  Loads Gemma3Model, generates text for multiple input prompts, and writes generation outputs to a JSON file.
  Accepts test_data_json as either (A) a JSON string (one-line) or (B) a path to a JSON file.
inputs:
  - name: tokenizer_json
    type: Model
    description: Path to tokenizer JSON file
  - name: model_py
    type: Data
    description: Path to model Python code file (or directory containing a single .py)
  - name: model_config
    type: Data
    description: Path to model config JSON file
  - name: learned_weights
    type: Model
    description: Path to learned model weights file
  - name: test_data_json
    type: String
    description: JSON array string (one-line) OR a path to a JSON file containing the array of prompts
  - name: max_new_tokens
    type: Integer
    default: "64"
    description: Maximum number of tokens to generate
  - name: temperature
    type: Float
    default: "1.0"
    description: Sampling temperature (higher = more random, lower = more deterministic)
  - name: top_k
    type: Integer
    default: "50"
    description: Number of top tokens to sample from (top-k sampling)
outputs:
  - name: inference_results
    type: String
    description: JSON file with generation results (list of {inference_input, inference_output})
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        set -e
        pip install tokenizers torch || true
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, importlib.util, json, os, sys, shutil
        import torch
        from tokenizers import Tokenizer
        import torch.nn.functional as F

        def find_model_py_file(path_or_dir):
            # returns a single .py file path (if argument is a file, returns it)
            if os.path.isfile(path_or_dir):
                return path_or_dir
            if not os.path.isdir(path_or_dir):
                raise ValueError("model_py must be a file or a directory containing one .py file")
            py_files = [os.path.join(path_or_dir, f) for f in os.listdir(path_or_dir) if f.endswith(".py")]
            if not py_files:
                raise FileNotFoundError("No .py file found in directory: " + path_or_dir)
            return py_files[0]

        def load_module_from_path(py_path, mod_name="gemma3_model"):
            if not py_path.endswith(".py"):
                tmp = py_path + ".py"
                shutil.copyfile(py_path, tmp)
                py_path = tmp
            if not os.path.exists(py_path):
                raise FileNotFoundError("Model file not found: " + py_path)
            spec = importlib.util.spec_from_file_location(mod_name, py_path)
            if spec is None or spec.loader is None:
                raise ImportError("Cannot load module from: " + py_path)
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
            return module

        parser = argparse.ArgumentParser()
        parser.add_argument("--tokenizer_json", type=str, required=True)
        parser.add_argument("--model_py", type=str, required=True)
        parser.add_argument("--model_config", type=str, required=True)
        parser.add_argument("--learned_weights", type=str, required=True)
        parser.add_argument("--test_data_json", type=str, required=True)
        parser.add_argument("--max_new_tokens", type=int, default=64)
        parser.add_argument("--temperature", type=float, default=1.0)
        parser.add_argument("--top_k", type=int, default=50)
        parser.add_argument("--inference_results", type=str, required=True)
        args = parser.parse_args()

        # ensure output dir exists
        out_dir = os.path.dirname(args.inference_results) or "."
        os.makedirs(out_dir, exist_ok=True)

        # device + dtype
        device = "cuda" if torch.cuda.is_available() else "cpu"
        torch_device = torch.device(device)
        model_dtype = torch.float16 if device == "cuda" else torch.float32

        # load tokenizer
        try:
            tok = Tokenizer.from_file(args.tokenizer_json)
        except Exception as e:
            sys.stderr.write("ERROR: Failed to load tokenizer: " + str(e) + "\n")
            sys.exit(3)

        def token_id_any(tok_obj, candidates):
            for c in candidates:
                try:
                    tid = tok_obj.token_to_id(c)
                except Exception:
                    tid = None
                if tid is not None:
                    return tid
            return None

        bos_id = token_id_any(tok, ["<s>", "<bos>"])
        eos_id = token_id_any(tok, ["</s>", "<eos>"])

        # load model config
        try:
            with open(args.model_config, "r", encoding="utf-8") as f:
                cfg = json.load(f)
        except Exception as e:
            sys.stderr.write("ERROR: Failed to read model_config: " + str(e) + "\n")
            sys.exit(4)

        # ensure dtype in cfg; keep as string for portability
        cfg["dtype"] = "float16" if model_dtype == torch.float16 else "float32"

        # load model code
        try:
            model_py_path = find_model_py_file(args.model_py)
            mod = load_module_from_path(model_py_path, "gemma3_model")
            Gemma3Model = getattr(mod, "Gemma3Model")
        except Exception as e:
            sys.stderr.write("ERROR: Failed to load model code: " + str(e) + "\n")
            sys.exit(5)

        # instantiate and load weights
        try:
            model = Gemma3Model(cfg).to(torch_device)
            state = torch.load(args.learned_weights, map_location=torch_device)
            model.load_state_dict(state, strict=True)
            model.eval()
        except Exception as e:
            sys.stderr.write("ERROR: Failed to instantiate/load model: " + str(e) + "\n")
            sys.exit(6)

        context_len = int(cfg.get("context_length", 2048))

        def generate_text(prompt, max_new_tokens=64, temperature=1.0, top_k=50):
            ids = tok.encode(prompt).ids
            if bos_id is not None:
                ids = [bos_id] + ids
            x = torch.tensor([ids], dtype=torch.long, device=torch_device)
            for _ in range(max_new_tokens):
                with torch.no_grad():
                    model_output = model(x)[0]
                    logits = model_output[0, -1]
                logits = logits / (temperature if temperature > 0 else 1.0)
                if top_k is not None and top_k > 0:
                    kth = torch.topk(logits, min(top_k, logits.shape[-1])).values[-1]
                    indices_to_remove = logits < kth
                    logits[indices_to_remove] = -float("Inf")
                probs = F.softmax(logits, dim=-1)
                next_id = torch.multinomial(probs, 1).item()
                ids.append(next_id)
                if eos_id is not None and next_id == eos_id:
                    break
                if len(ids) > context_len:
                    ids = ids[-context_len:]
                x = torch.tensor([ids], dtype=torch.long, device=torch_device)
            dec_ids = ids[1:] if (bos_id is not None and len(ids) and ids[0] == bos_id) else ids
            try:
                return tok.decode(dec_ids)
            except Exception:
                # fallback: map ids to strings if tokenizer decode fails
                return " ".join(str(i) for i in dec_ids)

        # load test_data_json (either a file path or JSON string)
        td = args.test_data_json
        test_data = None
        if os.path.exists(td) and os.path.isfile(td):
            try:
                with open(td, "r", encoding="utf-8") as fh:
                    test_data = json.load(fh)
            except Exception as e:
                sys.stderr.write("ERROR: Failed to parse JSON file for test_data_json: " + str(e) + "\n")
                sys.exit(7)
        else:
            try:
                test_data = json.loads(td)
            except json.JSONDecodeError:
                # write a short diagnostic (single-line) and exit
                sample = repr(td[:1000])
                sys.stderr.write("ERROR: Invalid JSON passed to --test_data_json. Preview: " + sample + "\n")
                sys.exit(8)

        if not isinstance(test_data, list):
            sys.stderr.write("ERROR: test_data must be a JSON list of prompt strings.\n")
            sys.exit(9)

        results = []
        for input_text in test_data:
            try:
                generated_output = generate_text(input_text, args.max_new_tokens, args.temperature, args.top_k)
            except Exception as e:
                generated_output = "[ERROR during generation: " + str(e) + "]"
            results.append({"inference_input": input_text, "inference_output": generated_output})

        # save outputs
        try:
            with open(args.inference_results, "w", encoding="utf-8") as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
        except Exception as e:
            sys.stderr.write("ERROR: Failed to write inference_results: " + str(e) + "\n")
            sys.exit(10)
    args:
      - --tokenizer_json
      - {inputPath: tokenizer_json}
      - --model_py
      - {inputPath: model_py}
      - --model_config
      - {inputPath: model_config}
      - --learned_weights
      - {inputPath: learned_weights}
      - --test_data_json
      - {inputValue: test_data_json}
      - --max_new_tokens
      - {inputValue: max_new_tokens}
      - --temperature
      - {inputValue: temperature}
      - --top_k
      - {inputValue: top_k}
      - --inference_results
      - {outputPath: inference_results}
