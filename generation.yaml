name: SLM_Inference
description: Loads Gemma3Model and generates text for a list of prompts; writes JSON string to a String output
inputs:
  - name: tokenizer_json
    type: Model
    description: Path to tokenizer JSON file
  - name: model_py
    type: Data
    description: Path to model Python code file or a directory containing it
  - name: model_config
    type: Data
    description: Path to model config JSON file
  - name: learned_weights
    type: Model
    description: Path to learned model weights file
  - name: prompts_json
    type: String
    description: JSON string containing a list of prompt strings
  - name: max_new_tokens
    type: Integer
    default: "64"
    description: Maximum number of tokens to generate
  - name: temperature
    type: Float
    default: "1.0"
    description: Sampling temperature
  - name: top_k
    type: Integer
    default: "50"
    description: Sample only from top-k tokens
outputs:
  - name: generations_json
    type: String
    description: JSON string with prompts and generated outputs
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        set -e
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, importlib.util, json, os, torch
        from tokenizers import Tokenizer
        import torch.nn.functional as F

        def find_model_py_file(path):
            if os.path.isfile(path):
                return path
            if not os.path.isdir(path):
                raise ValueError(f"Path {path} is neither a file nor a directory")
            cands = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(".py")]
            if not cands:
                raise FileNotFoundError(f"No .py file found in directory {path}")
            return cands[0]

        def load_module_from_path(py_path, mod_name="gemma3_model"):
            if not py_path.endswith(".py"):
                tmp = py_path + ".py"
                import shutil
                shutil.copyfile(py_path, tmp)
                py_path = tmp
            if not os.path.exists(py_path):
                raise FileNotFoundError(f"Model file not found: {py_path}")
            spec = importlib.util.spec_from_file_location(mod_name, py_path)
            if spec is None or spec.loader is None:
                raise ImportError(f"Cannot load module from: {py_path}")
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
            return module

        def get_token_id(tok, *tokens):
            for t in tokens:
                i = tok.token_to_id(t)
                if i is not None:
                    return i
            return None

        parser = argparse.ArgumentParser()
        parser.add_argument("--tokenizer_json", type=str, required=True)
        parser.add_argument("--model_py", type=str, required=True)
        parser.add_argument("--model_config", type=str, required=True)
        parser.add_argument("--learned_weights", type=str, required=True)
        parser.add_argument("--prompts_json", type=str, required=True)
        parser.add_argument("--max_new_tokens", type=int, default=64)
        parser.add_argument("--temperature", type=float, default=1.0)
        parser.add_argument("--top_k", type=int, default=50)
        parser.add_argument("--generations_json", type=str, required=True)  # outputPath
        args = parser.parse_args()

        device = "cuda" if torch.cuda.is_available() else "cpu"
        torch_device = torch.device(device)
        model_dtype = torch.float16 if device == "cuda" else torch.float32

        tok = Tokenizer.from_file(args.tokenizer_json)
        bos_id = get_token_id(tok, "<s>", "<bos>")
        eos_id = get_token_id(tok, "</s>", "<eos>")

        with open(args.model_config, "r", encoding="utf-8") as f:
            cfg = json.load(f)
        cfg["dtype"] = model_dtype

        model_py_path = find_model_py_file(args.model_py)
        mod = load_module_from_path(model_py_path, "gemma3_model")
        Gemma3Model = getattr(mod, "Gemma3Model")

        model = Gemma3Model(cfg).to(torch_device)
        state = torch.load(args.learned_weights, map_location=torch_device)
        model.load_state_dict(state, strict=True)
        model.eval()

        context_len = int(cfg.get("context_length", 2048))

        def generate_text(prompt, max_new_tokens=64, temperature=1.0, top_k=50):
            ids = tok.encode(prompt).ids
            if bos_id is not None:
                ids = [bos_id] + ids
            x = torch.tensor([ids], dtype=torch.long, device=torch_device)

            steps = max(0, int(max_new_tokens))
            temp = max(1e-5, float(temperature))

            for _ in range(steps):
                with torch.no_grad():
                    logits = model(x)[0][0, -1]

                logits = logits / temp

                if top_k is not None and top_k > 0:
                    k = min(int(top_k), logits.shape[0])
                    thresh = torch.topk(logits, k).values[-1]
                    logits[logits < thresh] = -float("inf")

                probs = F.softmax(logits, dim=-1)
                next_id = torch.multinomial(probs, 1).item()
                ids.append(next_id)

                if eos_id is not None and next_id == eos_id:
                    break

                if len(ids) > context_len:
                    ids = ids[-context_len:]
                x = torch.tensor([ids], dtype=torch.long, device=torch_device)

            dec_ids = ids[1:] if (bos_id is not None and len(ids) and ids[0] == bos_id) else ids
            return tok.decode(dec_ids)

        raw = json.loads(args.prompts_json)
        prompts = raw if isinstance(raw, list) else [str(raw)]
        results = [{"prompt": p, "output": generate_text(p, args.max_new_tokens, args.temperature, args.top_k)} for p in prompts]

        os.makedirs(os.path.dirname(args.generations_json) or ".", exist_ok=True)
        with open(args.generations_json, "w", encoding="utf-8") as f:
            f.write(json.dumps(results, ensure_ascii=False))

args:
  - --tokenizer_json
  - {inputPath: tokenizer_json}
  - --model_py
  - {inputPath: model_py}
  - --model_config
  - {inputPath: model_config}
  - --learned_weights
  - {inputPath: learned_weights}
  - --prompts_json
  - {inputValue: prompts_json}
  - --max_new_tokens
  - {inputValue: max_new_tokens}
  - --temperature
  - {inputValue: temperature}
  - --top_k
  - {inputValue: top_k}
  - --generations_json
  - {outputPath: generations_json}
