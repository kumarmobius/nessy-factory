name: HFConvertor
description: >
  Convert a GPT-NeoX checkpoint + tokenizer/config into a Hugging Face model folder.
  Uses named argparse flags. Core conversion logic is preserved from your uploaded script.

inputs:
  - name: tokenizer_json
    type: Model
    description: Path to tokenizer JSON file (optional)
  - name: model_pt
    type: Model
    description: Path to model PyTorch checkpoint file (required)
  - name: model_config_json
    type: Data
    description: Path to model config JSON file (optional)
  - name: cache_dir
    type: String
    default: /tmp/hf_cache
    description: Cache directory for conversion
  - name: output_model_dir
    type: String
    default: /tmp/hf_model
    description: Intermediate output model directory (HF-format)
  - name: max_new_tokens
    type: Integer
    default: 80
    description: Maximum new tokens (kept for parity)

outputs:
  - name: converted_model
    type: Model
    description: Path to converted HuggingFace model (artifact directory)

implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - bash
      - -lc
      - |
        cat > /tmp/convert_to_hf.py <<'EOF'
        #!/usr/bin/env python3
        import argparse, os, re, json, shutil, torch
        from collections import OrderedDict

        parser = argparse.ArgumentParser(description="convert_to_hf (parameterized)")
        parser.add_argument("--src_pt", required=True, help="Source checkpoint .pt/.bin")
        parser.add_argument("--tokenizer_json", default="", help="tokenizer.json (optional)")
        parser.add_argument("--base_cfg_json", default="", help="base config JSON (optional)")
        parser.add_argument("--model_dir", default="/tmp/hf_model", help="Intermediate HF model dir to write")
        parser.add_argument("--out_converted", default="/tmp/outputs/converted_model", help="Artifact output dir")
        args = parser.parse_args()

        SRC_PT = args.src_pt
        TOKENIZER_JSON = args.tokenizer_json
        BASE_CFG_JSON = args.base_cfg_json
        MODEL_DIR = args.model_dir
        OUT_CONVERTED = args.out_converted

        os.makedirs(MODEL_DIR, exist_ok=True)

        sd = torch.load(SRC_PT, map_location="cpu")
        if isinstance(sd, dict) and "state_dict" in sd:
            sd = sd["state_dict"]

        def normalize_keys(orig_sd):
            od = OrderedDict(orig_sd)
            has_prefix = any(k.startswith("gpt_neox.") for k in od.keys())
            new_od = OrderedDict()
            if not has_prefix:
                for k, v in od.items():
                    new_od["gpt_neox." + k] = v
            else:
                new_od = OrderedDict(od)
            replacements = {
                ".attention.qkv_proj.": ".attention.query_key_value.",
                ".attention.out_proj.": ".attention.dense.",
                ".final_layernorm.": ".final_layer_norm.",
            }
            remapped = OrderedDict()
            for k, v in new_od.items():
                newk = k
                for old, new in replacements.items():
                    if old in newk:
                        newk = newk.replace(old, new)
                remapped[newk] = v
            return remapped

        out = normalize_keys(sd)

        if "gpt_neox.embed_out.weight" not in out and "gpt_neox.embed_in.weight" in out:
            out["gpt_neox.embed_out.weight"] = out["gpt_neox.embed_in.weight"]

        layer_ids = sorted({int(m.group(1)) for k in out.keys()
                            for m in [re.match(r"gpt_neox\.layers\.(\d+)\.attention\.query_key_value\.weight$", k)] if m})
        for i in layer_ids:
            wq = out.get(f"gpt_neox.layers.{i}.attention.query_key_value.weight")
            wo = out.get(f"gpt_neox.layers.{i}.attention.dense.weight")
            if wq is not None and f"gpt_neox.layers.{i}.attention.query_key_value.bias" not in out:
                out[f"gpt_neox.layers.{i}.attention.query_key_value.bias"] = torch.zeros(wq.size(0), dtype=wq.dtype)
            if wo is not None and f"gpt_neox.layers.{i}.attention.dense.bias" not in out:
                out[f"gpt_neox.layers.{i}.attention.dense.bias"] = torch.zeros(wo.size(0), dtype=wo.dtype)

        torch.save(out, os.path.join(MODEL_DIR, "pytorch_model.bin"))

        vocab_size, hidden_size = out["gpt_neox.embed_in.weight"].shape
        num_layers = 1 + max(int(re.search(r"gpt_neox\.layers\.(\d+)\.", k).group(1))
                             for k in out if k.startswith("gpt_neox.layers."))
        intermediate_size = out["gpt_neox.layers.0.mlp.dense_h_to_4h.weight"].shape[0]

        def infer_heads(hid):
            for h in [32, 16, 8, 24, 12, 4, 2]:
                if hid % h == 0: return h
            return max(1, hid // 64) or 8

        def detect_tie(sd_map):
            try:
                return sd_map["gpt_neox.embed_out.weight"].data_ptr() == sd_map["gpt_neox.embed_in.weight"].data_ptr()
            except Exception:
                return False

        cfg = {}
        if BASE_CFG_JSON and os.path.exists(BASE_CFG_JSON):
            try:
                with open(BASE_CFG_JSON) as f: cfg = json.load(f)
            except Exception:
                cfg = {}

        num_heads = int(cfg.get("num_attention_heads") or cfg.get("num_heads") or infer_heads(hidden_size))
        max_pos = int(cfg.get("max_position_embeddings") or cfg.get("max_seq_len") or 512)
        rotary_pct = float(cfg.get("rotary_pct", 0.25))

        tie_word_embeddings = bool(cfg.get("tie_word_embeddings", detect_tie(out)))

        hf_cfg = {
            "architectures": ["GPTNeoXForCausalLM"],
            "model_type": "gpt_neox",
            "vocab_size": vocab_size,
            "hidden_size": hidden_size,
            "intermediate_size": intermediate_size,
            "num_attention_heads": num_heads,
            "num_hidden_layers": num_layers,
            "max_position_embeddings": max_pos,
            "rotary_pct": rotary_pct,
            "tie_word_embeddings": tie_word_embeddings,
        }

        with open(os.path.join(MODEL_DIR, "config.json"), "w") as f:
            json.dump(hf_cfg, f, indent=2)

        if tie_word_embeddings and "gpt_neox.embed_out.weight" in out:
            out.pop("gpt_neox.embed_out.weight")
            torch.save(out, os.path.join(MODEL_DIR, "pytorch_model.bin"))

        ids = {"pad_token_id": None, "unk_token_id": None, "bos_token_id": None, "eos_token_id": None}
        if TOKENIZER_JSON and os.path.exists(TOKENIZER_JSON):
            shutil.copy2(TOKENIZER_JSON, os.path.join(MODEL_DIR, "tokenizer.json"))
            try:
                t = json.load(open(TOKENIZER_JSON))
                name_map = {"[PAD]":"pad_token_id","[UNK]":"unk_token_id","[BOS]":"bos_token_id","[EOS]":"eos_token_id"}
                for a in t.get("added_tokens", []):
                    if a.get("special") and "id" in a and a.get("content") in name_map:
                        ids[name_map[a["content"]]] = a["id"]
            except Exception:
                ids = {}
        else:
            print("Warning: tokenizer.json not found; skipping copy.")
            ids = {}

        tok_cfg = {
            "model_max_length": max_pos,
            **{k: ids.get(k, v) for k, v in {
                "pad_token_id": 0, "unk_token_id": 1, "bos_token_id": 2, "eos_token_id": 3
            }.items()}
        }
        with open(os.path.join(MODEL_DIR, "tokenizer_config.json"), "w") as f:
            json.dump(tok_cfg, f, indent=2)

        spec_map = {
            "pad_token": "[PAD]", "unk_token": "[UNK]", "bos_token": "[BOS]", "eos_token": "[EOS]"
        }
        with open(os.path.join(MODEL_DIR, "special_tokens_map.json"), "w") as f:
            json.dump(spec_map, f, indent=2)

        gen_cfg = {"max_length": max_pos, "do_sample": False, "eos_token_id": tok_cfg["eos_token_id"]}
        with open(os.path.join(MODEL_DIR, "generation_config.json"), "w") as f:
            json.dump(gen_cfg, f, indent=2)

        print("[CONVERT] Copying HF folder to artifact dir:", OUT_CONVERTED)
        os.makedirs(OUT_CONVERTED, exist_ok=True)
        for root, dirs, files in os.walk(MODEL_DIR):
            rel = os.path.relpath(root, MODEL_DIR)
            target_dir = os.path.join(OUT_CONVERTED, rel) if rel != "." else OUT_CONVERTED
            os.makedirs(target_dir, exist_ok=True)
            for fname in files:
                shutil.copy2(os.path.join(root, fname), os.path.join(target_dir, fname))

        print("[CONVERT] Conversion completed successfully.")
        print("[CONVERT] Artifact available at:", OUT_CONVERTED)
        EOF
        python3 -u /tmp/convert_to_hf.py "$@"
    args:
      - --src_pt
      - {inputPath: model_pt}
      - --tokenizer_json
      - {inputPath: tokenizer_json}
      - --base_cfg_json
      - {inputPath: model_config_json}
      - --model_dir
      - {inputValue: output_model_dir}
      - --out_converted
      - {outputPath: converted_model}
